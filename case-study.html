<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="UTF-8" />
    <title>Venice</title>

    <link rel="stylesheet" href="stylesheets/reset.css" />
    <link rel="stylesheet" href="stylesheets/main.css" />

    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="images/icons/favicons/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="images/icons/favicons/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="images/icons/favicons/favicon-16x16.png"
    />
    <link rel="manifest" href="images/icons/favicons/manifest.json" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      href="https://fonts.googleapis.com/css2?family=Barlow+Condensed:wght@600&family=Fira+Code&family=Open+Sans:ital,wght@0,400;0,600;1,400&display=swap"
      rel="stylesheet"
    />
    <meta property="og:title" content="Venice Case Study" />
    <meta
      property="og:description"
      content="An open-source framework for deploying and managing stream processing pipelines"
    />
    <meta property="og:image" content="/images/thumbnail.png" />
    <meta name="theme-color" content="#0f4c75" />
    <script src="javascripts/sidebar.js"></script>
  </head>

  <body>
    <header class="header-short">
      <nav>
        <ul>
          <li>
            <a href="index.html">
              <img src="images/logos/venice_logo.png" alt="Venice logo" />
            </a>
          </li>
          <li><a href="case-study.html">Case Study</a></li>
          <li><a href="docs.html">Docs</a></li>
          <li><a href="team.html">Team</a></li>
          <li class="flex-float-right">
            <a href="https://github.com/venice-framework" target="_blank">
              <img
                src="images/logos/github-mark-light.png"
                alt="Venice GitHub"
                class="github"
              />
            </a>
          </li>
        </ul>
      </nav>
    </header>

    <div class="study-wrapper">
      <aside class="sidebar">
        <ul>
          <li>
            <a href="#Introduction"> 1. Introduction</a>
          </li>
          <li>
            <a href="#Background"> 2. Background</a>
          </li>
          <li>
            <a href="#Venice"> 3. Venice</a>
          </li>
          <li>
            <a href="#Architecture"> 4. Architecture & Deployment</a>
          </li>
          <li>
            <ul>
              <li>
                <a href="#Docker">
                  4.1 Docker
                </a>
              </li>
              <li>
                <a href="#Kafka-UI">
                  4.2 Kafka UI - Kafdrop
                </a>
              </li>
              <li>
                <a href="#Venice-CLI">
                  4.3 Venice CLI
                </a>
              </li>
            </ul>
          </li>
          <li>
            <a href="#Implementation"> 5. Implementation</a>
          </li>
          <li>
            <ul>
              <li>
                <a href="#Kafka">
                  5.1 Kafka
                </a>
              </li>
              <li>
                <a href="#ksqlDB">
                  5.2 ksqlDB
                </a>
              </li>
              <li>
                <a href="#Kafka-Connect">
                  5.3 Kafka Connect
                </a>
              </li>
              <li>
                <a href="#Schema-Registry">
                  5.4 Schema Registry
                </a>
              </li>
            </ul>
          </li>
          <li>
            <a href="#Challenges"> 6. Challenges</a>
          </li>
          <li>
            <ul>
              <li>
                <a href="#Data-Persistence"> 6.1 Data Persistence</a>
              </li>
              <li>
                <a href="#Containers"> 6.2 Containers</a>
              </li>
              <li>
                <a href="#Serialization-Format">
                  6.3 Data Serialization Formats</a
                >
              </li>
            </ul>
          </li>
          <li>
            <a href="#Future-Work"> 7. Conclusion & Future Work</a>
          </li>
          <li>
            <a href="#References"> 8. References</a>
          </li>
        </ul>
      </aside>

      <main>
        <section id="case-study">
          <h1>Case Study</h1>

          <h2 id="Introduction" class="sec-header">1. Introduction</h2>

          <p>
            One of the main challenges for event-driven microservices
            architecture is how the system handles the volume and velocity of
            data that needs to be distributed to different services. Many
            services may rely on the same event data but require it in different
            formats. One solution, event-stream processing, involves processing
            and transforming events as they arrive in the system before
            distributing them to their respective services. Event-stream
            processing decouples data writes from data reads. This means we can
            choose the ideal tools and formats for both reads and writes.
            Event-stream processing is a powerful solution for delivering event
            data in real time by centralizing event transformations in a single
            pipeline.
          </p>

          <p>
            The implementation of these pipelines differs greatly, but
            essentially they have several components that are connected to
            achieve the following workflow:
          </p>

          <ol>
            <li>Event producers generate event messages</li>
            <li>
              The events are transmitted from the producer to the message broker
            </li>
            <li>
              From the broker, they are consumed by the stream-processing engine
              for transformation
            </li>
            <li>Event consumers read the transformed event data</li>
          </ol>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Pipeline-Generalized.png"
              alt="data source feeds message broker feeds stream processing engine feeds data sink"
            />
            <figcaption>
              Figure 1. A high-level overview of a generalized event-stream.
              processing pipeline
            </figcaption>
          </figure>

          <p>
            Developing and deploying an event-stream processing pipeline is
            typically expensive, complex, or both. Venice is an open-source
            framework that enables small teams starting out with event-stream
            processing to quickly deploy and manage an event-stream processing
            pipeline.
          </p>
          <p>
            Venice is built for smaller applications and for developers with
            limited event-streaming knowledge. It uses open source components to
            set up a stream processing pipeline with reasonable default settings
            and simple management tools within minutes.
          </p>
          <p>
            Developers building their first event-stream processing applications
            face some key challenges:
          </p>

          <ol>
            <li>
              Choosing between a number of potential components and creating a
              functioning pipeline.
            </li>
            <li>
              Connecting services to the pipeline to read the processed data.
            </li>
            <li>
              Ensuring consistency and correctness of data at all stages of the
              pipeline.
            </li>
            <li>
              Persisting event data and transformed data to populate
              materialized views and add new sinks as requirements change.
            </li>
          </ol>
          <p>
            Venice abstracts away much of this complexity so that developers can
            focus on application code, rather than building and future-proofing
            this pipeline. Venice is a fully extensible foundation from which
            small streaming applications can grow as requirements shift.
          </p>

          <p>
            This case study outlines the design, architecture, and
            implementation challenges of Venice. The value of Venice may become
            more apparent with insight into some of the problems that events and
            event-stream processing solve for distributed web applications.
          </p>

          <h2 id="Background" class="sec-header">2. Background</h2>

          <p>
            Many applications start out as simple monoliths. Over time, as
            requirements grow and become more complex, the need for scale
            prompts many to shift to a service-oriented architecture, such as
            microservices [<a href="#microservices">1, Ch. 1</a>].
          </p>

          <p>
            One of the goals of microservices is to decouple services from each
            other. However, multiple services often require access to the same
            underlying data, including historical data.
          </p>

          <p>
            How might an organization
            <b>propagate data to multiple services in real-time</b> without
            sacrificing <b>availability</b><sup>1</sup> or <b>consistency</b>,
            while retaining the flexibility to <b>use historical data</b>?
          </p>

          <p class="note">
            <sup>1</sup>Availability here means the opposite of downtime. If a
            system is responsive it is available.
          </p>

          <h3>
            2.1 It is difficult to share data between distributed services
          </h3>

          <p>
            An application might resemble this: a jumble of requests and data
            propagations between the application, a primary database, data
            warehouse, business analytics engine, search index, cache, and graph
            database.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/PrimaryDatabaseDependency.png"
              style="max-height: 400px;"
              alt="A web application sends data to a primary database that feeds multiple other databases and processes"
            />
            <figcaption>
              Figure 2. The same data is needed for many different purposes.
              <sup>1</sup>
            </figcaption>
          </figure>

          <p class="note">
            <sup>1</sup> Figure inspired by [<a href="#mssp">2, Ch 2</a>].
          </p>

          <p>
            There are numerous ways to use data from this application, and
            different tools are better suited for different situations. For
            example, an e-commerce application might use orders data to
            continuously update:
          </p>
          <ul>
            <li>
              A cache of “deals of the day” and remaining inventory, to avoid
              overloading the database with redundant queries.
            </li>
            <li>
              A search index, to enable full-text search of available products.
            </li>
            <li>
              A key-value store of personalised product recommendations for
              customers.
            </li>
          </ul>

          <p>
            These outputs are all <b>derived views</b> of the same underlying
            data. Typically, it is difficult to maintain real-time services –
            such as the stock inventory, search index, or product recommendation
            services from above –
            <b
              >without sacrificing the availability of the services or
              consistency of data</b
            >. When a product runs out of stock, this change would ideally be
            propagated instantly to all these services, instead of hourly or
            daily. From a user and business perspective, it is desirable to
            update the shop as close to real time as possible. This would
            prevent displaying out-of-stock products to users.
          </p>

          <p>
            Dual writes are one way to update data across multiple services.
            However, partial failures, such as a network outage, can result in
            permanent inconsistency. Imagine writing orders to one database and
            updating remaining inventory in a different database. If a new order
            comes in for Product X, and is written successfully to the orders
            database, but the network crashes before the remaining inventory is
            updated, the two data stores are now inconsistent, and will remain
            that way without a manual recount.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/DualWriteFailure.png"
              style="max-height: 400px;"
              alt="A customer order is written to one database but the accompanying inventory update fails"
            />
            <figcaption>
              A successful write to one database and a failed write to the
              second.
            </figcaption>
          </figure>

          <p>
            Distributed transactions can provide consistency, but may sacrifice
            availability, because partial failures can render the application
            extremely slow or unresponsive [<a href="mssp">2, Ch. 2</a>;
            <a href="two-phase-commit">3</a>].
          </p>

          <p>
            To address this challenge, some companies have turned to using
            events to propagate data changes between their services.
          </p>

          <h3>2.2 Events drive application behavior</h3>

          <p>
            An event is an immutable object that describes something that
            happened at some point in time in an application [<a href="#ddia"
              >4, Ch. 11</a
            >]. For example, if a new order is placed, that order can be modeled
            as an event as follows:
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/OrderEventObject.png"
              style="max-height: 400px;"
              alt="order event object with properties for order_id, customer_id, seller_id, product_id, quantity, price_in_cents, timestamp"
            />
            <figcaption>Figure 3. An example of an order event.</figcaption>
          </figure>

          <p>
            An event can trigger one or more actions. For example, when a new
            order comes in, the event can trigger an update to the inventory,
            start the fulfillment process, and send an email notification to the
            customer.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/OrderTriggersServices.png"
              style="max-height: 400px;"
              alt="data from the order object in the previous figure is sent to three different services"
            />
            <figcaption>
              Figure 4. A new order triggers the inventory management,
              fulfillment, and notifications services.
            </figcaption>
          </figure>

          <p>
            In streaming terminology, a producer generates an event. Related
            events are grouped together into a stream (also known as a topic).
            That event is then processed by one or many consumers [<a
              href="#ddia"
              >4, Ch. 11</a
            >]. In the example above, the application is the producer. When a
            customer creates a new order, the application produces an event to
            the orders stream. The inventory management, fulfillment and
            notifications services are the consumers who receive the event and
            take appropriate action.
          </p>

          <h3>2.3 Message brokers move data from producers to consumers</h3>

          <p>Message brokers are suited for situations where:</p>

          <ul>
            <li>producers and consumers are asynchronous.</li>
            <li>
              multiple producers may write to a topic, and multiple consumers
              may read from a topic.
            </li>
          </ul>

          <p>
            There are two common types of message brokers: (1) message queues
            (brokers that implement the AMQP and JMS standards) and (2)
            log-based message brokers.
          </p>

          <h4>
            2.3.1 Message queues do not retain events and may process events out
            of order
          </h4>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/gifs/MessageQueue.gif"
              style="max-height: 400px;"
              alt="gif of a message queue with messages enqueing and dequeuing"
            />
            <figcaption>
              Figure 5. A message queue: new messages enqueue and old messages
              dequeue.
            </figcaption>
          </figure>

          <p>
            Message queues (e.g., RabbitMQ, ActiveMQ) are preferable in
            situations where:
          </p>

          <ul>
            <li>each event may take a long time to process.</li>
            <li>processing order is not important.</li>
            <li>event retention is not required.</li>
          </ul>

          <p>
            Recall that the motivating question was: How might an organization
            propagate data to multiple services in real-time without sacrificing
            availability or consistency, while retaining the flexibility to use
            historical data?
          </p>

          <p>
            Message queues are not the answer because they may process events
            out of order and do not retain events [<a href="#ddia">4, Ch. 11</a
            >].
          </p>

          <h4>
            2.3.2 Log-based message brokers retain events and help guarantee
            order
          </h4>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Log.png"
              style="max-height: 400px;"
              alt="a row of numbers in boxes, each box represents a message written to the log"
            />
            <figcaption>
              Figure 6. A log-based message broker: new messages are appended to
              an immutable log.
            </figcaption>
          </figure>

          <p>
            Log-based message brokers (e.g., Apache Kafka, Amazon Kinesis
            Streams) are preferable in situations where:
          </p>

          <ul>
            <li>each event can be processed quickly.</li>
            <li>processing order is important.</li>
            <li>
              event retention is required [<a href="#mssp">2, Ch. 2</a>;
              <a href="#ddia">4, Ch. 11</a>].
            </li>
          </ul>

          <p>
            Log-based message brokers are the answer to the motivating question.
          </p>

          <ol>
            <li>
              <b>Data can be propagated to services in real-time.</b><br />
              Logs support low latency and high throughput writes and reads.<br />
              <ul>
                <li>
                  Low Latency: Each write is an append operation, and a read is
                  a linear scan. Producers can write data to the log in a single
                  step, and consumers can read data from the log very quickly.
                </li>
                <li>
                  High Throughput: Logs can be split into partitions and
                  replicated across machines. This means many reads and writes
                  can proceed concurrently [<a href="#okay-store-data">5</a>].
                </li>
              </ul>
            </li>
            <li>
              <b>The system is fault-tolerant.</b>
              <ul>
                <li>
                  With multiple brokers, messages are rerouted to replicas if
                  one crashes.
                </li>
                <li>
                  With partitioned data and load-balanced consumers, if one
                  consumer fails, others can pick up the work.
                </li>
              </ul>
            </li>
            <li>
              <b>Data is consistent across services.</b>
              <ul>
                <li>
                  Events are ordered within partitions. Keyed partitions route
                  events with the same key to the same partition, ensuring
                  order. For example, partitioning by product ID allows all
                  orders for Product X to be processed in the order they are
                  produced.
                </li>
                <li>
                  Logs are immutable. Services that require the same underlying
                  data will all see the same data. This avoids inconsistencies
                  due to race conditions associated with having multiple
                  databases.
                </li>
              </ul>
            </li>
            <li>
              <b>Historical data is preserved.</b>
              <ul>
                <li>
                  Logs contain historical information and new information. New
                  services can be added to the system at any time, consume the
                  historical data, and follow future events automatically.
                </li>
                <li>
                  Developing new services is also possible, with the option to
                  gradually transition, or roll back changes.
                </li>
              </ul>
            </li>
          </ol>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/LogAsBroker.png"
              style="max-height: 400px;"
              alt="generic data sources send data to a log-based message broker that feeds into a data sink"
            />
            <figcaption>
              Figure 7. Event producers send messages to a partitioned log-based
              broker. Event consumers use these event messages in various ways.
            </figcaption>
          </figure>

          <h4>2.3.3 A note on the limitations of logs</h4>
          <p>
            Log-based approaches are not the answer to all problems. Events are
            consumed in a linear fashion, so a single event taking a long time
            to process could delay processing of later events. [<a href="#ddia"
              >4, Ch. 11</a
            >]. Ordering of events is only guaranteed within an individual
            partition, which may be problematic as the system scales. The system
            also does not support reading self writes [<a href="#ddia"
              >4, Ch. 5</a
            >], and other linearizability guarantees [<a href="#ddia"
              >4, Ch.9, Ch. 12</a
            >]. In many cases, the benefits outweigh the drawbacks, and
            log-based message brokers present a powerful solution to the problem
            of <b>propagating data to multiple services in real-time</b>,
            without sacrificing <b>availability</b> and <b>consistency</b>,
            while retaining the flexibility to <b>use historical data</b>.
          </p>

          <h3>
            2.4 Stream processors abstract common operations out to a fast and
            fault-tolerant engine
          </h3>

          <p>
            It is common to have multiple services use the same data to generate
            different outputs in an application. The outputs generated may be
            different, but many of the operations used by the services to
            process these events are the same. These operations include
            windowing, aggregation, joins, filters, and transformations [<a
              href="#streaming-systems"
              >6</a
            >].
          </p>

          <p>
            Individual consumers can be written to perform these operations.
            This works well for operations such as filtering or transforming
            that process one message at a time. This approach becomes more
            challenging when operations are more complex, for example,
            aggregating over time or joining two different streams together.
          </p>

          <p>
            Event stream processors abstract these common processing operations
            to an engine. Such an engine can provide additional benefits such as
            fault tolerance, efficient processing of events via clusters,
            maintenance of local state, and out-of-order events handling [<a
              href="kafka-streams"
              >7</a
            >].
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/gifs/ProcessorReplacesConsumers.gif"
              style="max-height: 400px;"
              alt="gif of several consumers being replaced by a single stream processing engine"
            />
            <figcaption>
              Figure 8. An event-stream processing engine replaces the need for
              individual consumers performing redundant processing tasks.
            </figcaption>
          </figure>

          <p>
            Adding a stream processor to a log-based architecture expands the
            benefits of the system. Services can write data efficiently by
            appending it to the log. The stream processor can complete
            operations shared by different services more efficiently. This means
            that services can share the same underlying data and easily
            transform that data into a format that is ideal for their needs.
          </p>

          <h4>2.4.1 A note on the importance of databases</h4>

          <p>
            This background emphasizes the benefits of an event-driven
            architecture with a distributed log at its core. However, event
            driven architecture does not replace databases completely.
          </p>

          <p>
            Databases prioritize state, while events prioritize state changes.
            State is derived from events – for example, your physical location
            as you read this is the result of the event that moved you there. In
            an application, both events and state are useful. Databases in an
            architecture provide fast access to state.
          </p>

          <p>
            A log-based, event-driven architecture utilizes distributed logs and
            databases to provide the benefits of both in a fault-tolerant and
            scalable way.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Brokers-Processor.png"
              style="max-height: 400px;"
              alt="producers or sources such as user activity, sensor data, or database writes feed events to a replicated log-based message broker which sends them to a stream processing engine and eventually they wind up in various consumers or sinks such as a cache, graph database, or search index"
            />
            <figcaption>
              Figure 9. An example of a log-based stream processing
              architecture.
            </figcaption>
          </figure>

          <h3>2.5 Summary</h3>

          <ul>
            <li>
              In a microservices architecture, many services
              <b>require access to the same data</b>.
            </li>
            <li>
              Propagating data to many services is difficult with architecture
              centered on <b>databases</b>.
            </li>
            <li>
              <b>Events</b> can help address this problem. Events represent what
              happened in the application and when.
            </li>
            <li>
              <b>Message brokers</b> help move events between
              <b>producers</b> (data creators, such as an application) and
              <b>consumers</b> (services that need the data).
            </li>
            <li>
              <b>Log-based message brokers</b> can propagate data to services in
              real time in a fault-tolerant, consistent, and scalable way, and
              provide the flexibility to use historical data.
            </li>
            <li>
              <b>Event stream processors</b> abstract out common operations on
              event data to a fault-tolerant and scalable engine that maintains
              event order.
            </li>
            <li>
              An architecture with a distributed event log at its core and
              databases to store outputs provides
              <b>fast access to both events and state</b>. Multiple services can
              now use the same data in different ways, and the application has
              the agility to grow with changing requirements.
            </li>
          </ul>

          <h2 id="Venice" class="sec-header">
            3. Venice
          </h2>

          <p>
            Venice is an open-source framework that enables developers starting
            out with event-stream processing to quickly deploy and manage an
            event-stream processing pipeline.
          </p>

          <p>
            Venice is built for smaller applications and for developers with
            limited event-streaming knowledge. It uses open source components to
            set up a stream processing pipeline with reasonable default settings
            and simple management tools within minutes.
          </p>

          <p>
            Large companies typically use stream processing to perform the
            following tasks:
          </p>

          <ul>
            <li>
              Process complex real-time event data to make applications more
              responsive.
            </li>
            <li>Detect anomalies in events.</li>
            <li>
              Deliver real-time analytics without interfering with the
              application’s ability to write new data.
            </li>
          </ul>

          <p>
            The desire to perform these tasks, however, is not unique to large
            enterprises with applications that process millions of events per
            day. Any growing application might benefit from event stream
            processing. The implementation of these pipelines differs, but they
            have several interconnected components that achieve the following
            workflow:
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Pipeline-Generalized.png"
              alt="one or more event processors (data sources) feed into a message broker (typically Kafka - a log) which feeds into a stream processing engine ending at event consumers also known as data sinks (visualization tools, caches, databases, Kafka topic, etc.)"
            />
            <figcaption>
              Figure 10. A generalized event-stream processing pipeline.
            </figcaption>
          </figure>

          <h3>3.1 Venice sits between DIY and managed solutions</h3>

          <p>
            The existing choices available to developers roughly fall into two
            categories: managed solutions and do-it-yourself (DIY).
          </p>

          <p>
            Managed solutions (such as Confluent’s Cloud Platform or Landoop)
            empower developers to focus on their applications by offering
            configuration guidance and on-going support for teams implementing
            streaming systems. However, they have high price tags, are generally
            designed for large enterprises, and can lock the team into using one
            vendor.
          </p>

          <p>
            The DIY approach is an option for teams newer to this space.
            However, it trades monetary cost for complexity. First, application
            developers must deal with an abundance of choice. For the stream
            processing engine alone, there are multiple open-source options and
            several paid ones (figure 11).
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/streamProcessingEngines.png"
              alt="logos for AWS Kinesis, Google CloudData, and Apache products: Samza, Spark, Flink, Apex, Flume, Heron, Storm, Kafka Streams, Beam, ksqlDB"
            />
            <figcaption>
              Figure 11. A sampling of the options for stream processing
              engines.
            </figcaption>
          </figure>

          <p>
            Second, because the stream-processing space is emerging,
            configuration tutorials become quickly outdated and documentation is
            minimal or challenging to parse. Choosing and learning how to
            configure a single component in the pipeline could take days or
            weeks. The problem compounds when attempting to learn which
            components best integrate together to form a pipeline with tradeoffs
            that are acceptable for the project.
          </p>

          <p>
            Existing managed solutions and open source options may not be
            suitable for cost-conscious developers with limited streaming
            knowledge developing small streaming applications. Venice aims to be
            the middle ground - balancing the need for affordability, simplicity
            and control.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/VeniceVsDIY-Managed.png"
              alt="Three arrows representing cost, ease of use, and user control with Venice, DIY, and managaged solutions arranged along them accordingly"
            />
            <figcaption>
              Figure 12. Venice positions itself between DIY and managed
              solutions in terms of cost, ease of use, and user control.
            </figcaption>
          </figure>

          <!-- style this section & add icons for clock and plus -->
          <h3>
            3.2 Design goals for an approachable stream processing framework
          </h3>

          <div class="design-goals">
            <p>
              Venice is approachable for developers new to event-stream
              processing. It allows them to:
            </p>
            <ul>
              <li>
                <!-- clock icon -->Set up common pipeline patterns in minutes so
                they can minimize time spent on configuration and focus on
                application code instead.
              </li>
              <li>
                <!-- plus icon -->Retain extensibility and potential for
                reconfiguration if their needs change or become more complex.
              </li>
            </ul>
          </div>

          <h2 id="" class="sec-header"></h2>

          <h2 id="" class="sec-header"></h2>

          <h2 id="Challenges" class="sec-header">6. Challenges</h2>

          <p>
            Venice solves for several challenges: persisting data, reliably
            launching and connecting component containers, and consistently
            serializing data.
          </p>

          <h3 id="Data-Persistence">
            6.1 How can Venice persist data to make materialized views and new
            services possible?
          </h3>
          <p>
            This challenge has three parts: data persistence, storage choice,
            and log retention strategy.
          </p>

          <h4>6.1.1 How should Venice persist data?</h4>

          <p>
            Retaining events indefinitely in Kafka allows users to build new
            services or re-design existing services that can consume the entire
            history of events to recreate application state. This is a critical
            feature for teams new to streaming applications who will be
            experimenting to discover what works.
          </p>

          Docker provides two ways to persist data for containers -
          <b>volumes</b> and <b>bind mounts</b>.

          <figure class="img-wrapper">
            <img
              src="images/diagrams/DockerVolumes_bindMounts.png"
              alt="An arrow points from the Docker container to the Docker area within the host machine created by a volume. A second arrow points to host machine in general to represent a bind mount."
            />
            <figcaption>
              Figure XX. Docker containers interact with the host machine’s file
              system through bind mounts or volumes.
            </figcaption>
          </figure>

          <p>
            The main difference, shown in figure xxxx, is where data is saved on
            the host machine. With volumes, Docker manages storing data in a
            restricted Docker directory. With a bind mount, a specific path on
            the host machine is loaded on the container at startup.
          </p>

          <p>
            Venice uses bind mounts to ensure Docker loads a specific path from
            the host machine into the container everytime a container starts.
            When using volumes, new volumes are created when containers are
            re-configured and rebuilt. Bind mounts are a more reliable and
            consistent way of reloading data into the container for developers
            experimenting with new pipelines.
          </p>

          <h4>
            6.1.2 What is the optimal way for Venice to provide event
            persistence?
          </h4>

          <p>
            There are three options for persisting Kafka events indefinitely:
            permanent log retention, external backups, and compacted topics.
          </p>

          <h5>Choice 1: Permanent Log Retention</h5>

          <p>
            Typically, Kafka will delete events after a predetermined amount of
            time (e.g., 7 days) or when the log reaches a certain byte size.
            However, permanently preserving events is possible by overriding the
            default retention settings. A new consumer could then consume
            everything from the beginning of the log to derive the current
            state.
          </p>

          <h5>Choice 2: External Backup</h5>

          <p>
            Another option is to periodically copy older events to an external
            data store and remove them from Kafka. To recreate state for a new
            consumer, the backed up data must be reloaded and configured in
            Kafka.
          </p>

          <h5>Choice 3: Compacted Topics</h5>

          <p>
            Finally, log compaction refers to retaining the last known value for
            each message <b>key</b> within a single topic partition. Compaction
            guarantees event order by only removing older events whose state has
            changed.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/TopicCompaction.png"
              alt="Two event objects. One has the email adddress 'daviddd@gmail.com'; the other, 'david@gmail.com'. A box around the second indicates it will be preserved."
            />
            <figcaption>
              Figure XX. An example of a user updating their email address. With
              a compacted topic only the latest value related to the
              <code>user_id: 1</code> is retained.
            </figcaption>
          </figure>

          <p>
            For example, figure xxx shows events that track a change to a user’s
            email, where the user_id is the key. With a compacted topic, Kafka
            would guarantee that the most recent update to the email would be
            kept in the log.
          </p>

          <p>
            Log compaction can be suitable for some topics. However, it means
            losing the ability to <b>recreate</b> state, leaving only the option
            to <b>restore</b> the latest state.
          </p>

          <h5>Venice: Permanent Log Retention</h5>

          <p>
            Venice uses permanent log retention because (1) it is a simpler
            solution than external backup, and (2) log compaction does not not
            fulfill the goal of extensibility. A compacted topic means users
            would not be able to consume historical data.
          </p>

          <p>
            Permanent log retention is a safe option: users have all of their
            events stored by default, but they can choose to turn on log
            compaction for individual topics if they decide that is appropriate
            for their use case.
          </p>

          <p>
            However, as a user’s application grows and events accumulate, they
            may want to consider external backup to free up space on the
            machines running the brokers.
          </p>

          <h3 id="Containers" class="sec-header">6.2 How can Venice reliably launch and connect containers?</h3>

          <p>Each component in the Venice pipeline launches in its own container. This poses a few challenges. 
          </p>

          <h4>6.2.1 How can Venice launch all of the containers with a single command?</h4>

          <p>A user would likely want to experiment with their pipeline, which means they would be starting and stopping the entire pipeline multiple times. Docker Compose allows Venice to define all of the pipeline components in a single YAML file and specify the startup order. A single command would start and stop all of the containers.</p>
          
          <h4>6.2.2 How can containers securely and reliably communicate with each other? </h4>

          <p>Docker Compose automatically creates a network for all components defined in the YAML file. This network enables communication between containers while isolating them from the host network. However, the name of this automatically generated network is dependent on the name of the directory where the YAML file is located[<a href="#docker-compose">11</a>, <a href="#compose-networking">12</a>]. For example, if the directory was called “venice”, the name of the default network would be “venice_default”. </p>
          <p>The Venice CLI needs to work with components on the network regardless of the names of the directories, so it uses Docker Compose’s custom networks feature to define a network named “venice”.</p>

          <h4>6.2.3 How can Venice address race conditions between containers?</h4>

          <p>Venice can dictate the startup order for each container, but sometimes this is not enough [<a href="#compose-startup">13</a>]. Just because a container is running does not mean it is listening on a port or ready to send or receive messages.</p>

          <p>Two problems stemming from this are: </p>

          <ol>
            <li>Producers crash if they attempt to connect to the Schema Registry before it is ready.</li>
            <li>The Venice <code>connector-init</code> service fails to initialize stored connectors on startup if Kafka Connect is not ready.</li>
          </ol>

          <p>
            Venice solves these dependency issues with shell scripts that ping for confirmation that the Schema Registry and Kafka Connect are ready to receive connections before running producers or initializing connectors [<a href="#rmoff-tricks">14</a>].
          </p>

          <h3 id="Serialization-Format">What default serialization format should Venice recommend?</h3>
          <p>Venice encourages developers to use <a href="https://avro.apache.org/docs/current/" target="_blank">Avro</a> but Avro keys are incompatible with ksqlDB.
          </p>

          <p>In Kafka, each event key and value is written as a pair of raw bytes. If a producer serializes the event in one format and the consumer deserializes it in another, the application will produce errors or the data will be unusable (see figure xxx). 
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/UnusableData.png"
              alt="screen capture of the Kafdrop UI showing message key and value as empty squares in place of characters"
            />
            <figcaption>
              Figure XX. Incorrect deserialization format causes unreadable key and value on Kafdrop.
            </figcaption>
          </figure>

          <p>ksqlDB requires <code>STRING</code> event keys, but the default configuration for many connectors is Avro keys and values. Encoding the key as a <code>STRING</code> and the value in Avro allows users to leverage all of ksqlDB’s functionalities while retaining many of the benefits of using Avro (since values typically contain more data than keys). </p>

          <hr />
          <!-- mothership text -->
          <p>These are problems of <em>service discovery:</em></p>

          <blockquote>
            <p>
              [A] modern microservice-based application typically runs in a
              virtualized or containerized environment where the number of
              instances of a service and their locations changes dynamically.
              Consequently, you must implement a mechanism that enables the
              clients of service to make requests to a dynamically changing set
              of ephemeral service instances.
              <a
                href="https://microservices.io/patterns/server-side-discovery.html"
                target="_blank"
                rel="noopener"
                >link</a
              >
            </p>
          </blockquote>

          <p>
            Mothership uses Docker Machine to handle server provisioning. Docker
            Machine comes with “providers” for many IaaS providers (Digital
            Ocean, Linode, AWS, and many others). These providers do the work of
            sending the requisite API calls to the IaaS provider in question to
            get a server provisioned with Docker installed and the correct ports
            open. By using Docker Machine, we freed ourselves to work at a
            higher level of abstraction in implementing scaling. Once we have
            this new Docker host, we can send it an API request instructing it
            to register itself on our Convoy.<sup class="footnote-ref"
              ><a href="#fn9" id="fnref9">[9]</a></sup
            >
          </p>
<hr> 
<!-- mothership text preserved to see how they used the blockquote and internal return links -->

          <h2 id="Future-Work" class="sec-header">7. Future Work</h2>
          <p>
            Venice meets many of the needs of developers new to event-stream
            processing. Venice provides a quick-to-deploy framework that has
            much of the configuration work automated. This means developers can
            get to their tasks of connecting their own data producers and
            implementing queries over their data in minutes. They can also
            verify things are running smoothly using the graphical user
            interface and CLI.
          </p>
          <p>
            However, there are always areas for improvement. The following goals
            would likely add the most value to Venice:
          </p>
          <ul>
            <li>
              Automate deployment and management across distributed servers
            </li>
            <li>Enable auto-scaling (up and down) according to workload</li>
            <li>Add more default pipelines</li>
            <li>Add support for more connectors</li>
            <li>Enable external network storage</li>
            <li>Automate the process of adding producers to the pipeline</li>
          </ul>

          <h2 id="References" class="sec-header">8. References</h2>

          <ol>
            <li id="microservices">
              S. Newman, Building Microservices, 1st ed. Sebastopol, CA, USA.
              O’Reilly, 2015.
            </li>
            <li id="mssp">
              M. Kleppman, Making Sense of Stream Processing, 1st ed.
              Sebastopol, CA, USA. O’Reilly, 2016.
            </li>
            <li id="ddia">
              M. Kleppmann, Designing Data Intensive Applications, 1st ed.
              Sebastopol, CA, USA. O’Reilly, 2017.
            </li>
            <li id="two-phase-commit">
              “Consensus Protocols: Two-Phase-Commit”, The Paper Trail blog,
              <a
                href="https://www.the-paper-trail.org/post/2008-11-27-consensus-protocols-two-phase-commit/"
                target="_blank"
                >https://www.the-paper-trail.org/post/2008-11-27-consensus-protocols-two-phase-commit/</a
              >
              (April 13, 2020).
            </li>
            <li id="okay-store-data">
              J. Kreps, “It’s Okay To Store Data In Apache Kafka”, Confluent.io
              blog,
              <a
                href="https://www.confluent.io/blog/okay-store-data-apache-kafka/"
                target="_blank"
                >https://www.confluent.io/blog/okay-store-data-apache-kafka/</a
              >
              (April 16, 2020).
            </li>
            <li id="streaming-systems">
              T. Akidau, S. Chernyak, and R. Lax, Streaming Systems, 1st ed.
              Sebastopol, CA, USA. O’Reilly, 2018.
            </li>
            <li id="kafka-streams">
              J. Kreps, “Introducing Kafka Streams: Stream Processing Made
              Simple”, Confluent.io blog,
              <a
                href="https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/"
                target="_blank"
                >https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/</a
              >
              (April 14, 2020).
            </li>
            <li></li>
            <li></li>
            <li></li>
            <li id="docker-compose">Docker documentation: Docker Compose, <a href=”https://docs.docker.com/compose/” target=”_blank”>https://docs.docker.com/compose/</a> (April 12, 2020).
            </li>
            <li id="compose-networking">Docker documentation: Docker Compose networking, <a href=”https://docs.docker.com/compose/networking/” target=”_blank”>https://docs.docker.com/compose/networking/</a> (April 12, 2020).
            </li>
            <li id="compose-startup">Docker documentation: Docker Compose startup order, <a href=”https://docs.docker.com/compose/startup-order/” target=”_blank”>https://docs.docker.com/compose/startup-order/</a> (April 10, 2020).</li>
            <li id="rmoff-tips">R. Moffatt, “Docker Tips with KSQL and Kafka”, Professional Blog of Robin Moffatt, <a href=”https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-ksql-and-kafka/” target=”_blank”>https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-ksql-and-kafka/</a> (April 9, 2020).</li>
          </ol>

          <hr class="footnotes-sep" />

          <div class="footnotes">
            <ol class="footnotes-list">
              <li id="fn1" class="footnote-item">
                <p>
                  A container orchestrator has several other critical
                  responsibilities, as we’ll see in the next section on service
                  discovery. <a href="#fnref1" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn2" class="footnote-item">
                <p>
                  Officially, Docker Swarm was a separate product from Docker
                  that is now deprecated, while <em>Swarm Mode</em> is the
                  current container orchestrator built into Docker as of v1.12.
                  Whenever we say “Docker Swarm”, we are referring to “Docker in
                  Swarm Mode.” <a href="#fnref2" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn3" class="footnote-item">
                <p>
                  Manager nodes in Docker Swarm are convenient in that they can
                  serve dual-duty as worker nodes. This makes it possible to
                  have a cluster with a single node, which is how Mothership
                  operates on first setup.
                  <a href="#fnref3" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn4" class="footnote-item">
                <p>
                  There is a subtle distinction between a task and a container
                  that is immaterial to the present discussion.
                  <a href="#fnref4" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn5" class="footnote-item">
                <p>
                  Since containers are ephemeral, but database data is
                  persistent, we make use of Docker volumes to persist database
                  data. <a href="#fnref5" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn6" class="footnote-item">
                <p>
                  The implementation details of Docker overlay networks get
                  pretty hairy, but
                  <a
                    href="https://success.docker.com/article/networking#overlaydrivernetworkarchitecture"
                    target="_blank"
                    rel="noopener"
                    >this reference article</a
                  >
                  is a good place to start.
                  <a href="#fnref6" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn7" class="footnote-item">
                <p>
                  This is an oversimplification because there may be
                  <em>multiple</em> running containers for a given service.
                  Docker Swarm uses DNS round-robin load-balancing to distribute
                  requests to services among running containers.
                  <a href="#fnref7" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn8" class="footnote-item">
                <p>
                  Under the hood, Docker accomplishes this via heavy use of
                  Linux iptables rules.
                  <a href="#fnref8" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn9" class="footnote-item">
                <p>
                  We don’t go into details here due to space, but even here
                  there were surprising complexities. For example, scaling a
                  Convoy <em>down</em> requires telling a pod to leave the
                  swarm, and then telling the Convoy to delete that pod. This
                  requires two Docker API calls. Unfortunately, the call for the
                  pod to leave returns a response before it has fully left, but
                  the call for the Convoy to delete it can only succeed after it
                  has fully left – an unfortunate race condition with no clean
                  solution. <a href="#fnref9" class="footnote-backref">↩︎</a>
                </p>
              </li>
            </ol>
          </div>
        </section>
      </main>
    </div>
    <section class="lower-cta">
      <h2>Ready to get started?</h2>

      <div class="buttons">
        <a href="docs.html" class="button button-link button-wide banner-cta"
          >Docs</a
        >
        <a href="" class="button button-primary button-wide banner-cta"
          >Quick Start</a
        >
        <!-- TODO: add link to quick start -->
        <a
          href="https://github.com/venice-framework"
          target="_blank"
          class="button button-link banner-cta"
          >GitHub</a
        >
      </div>
    </section>

    <footer>
      <div class="footer-backdrop">
        <section id="our-team">
          <h2>Our Team</h2>
          <p>
            We are looking for opportunities.<br />If you liked what you saw and
            want to talk, please reach out!
          </p>
          <ul>
            <li class="individual">
              <img src="images/team/nancy.png" alt="Nancy Trinh" />
              <h3>Nancy Trinh</h3>
              <p>San Francisco, CA</p>
              <ul class="social-icons">
                <li>
                  <a
                    href="mailto:nancytrinh20@gmail.com?subject=Venice%20Project"
                  >
                    <img src="images/icons/email_icon.png" alt="email" />
                  </a>
                </li>
                <li>
                  <a href="http://github.com/nantrinh" target="_blank">
                    <img src="images/icons/website_icon.png" alt="website" />
                  </a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/nancytrinh/"
                    target="_blank"
                  >
                    <img src="images/icons/linked_in_icon.png" alt="linkedin" />
                  </a>
                </li>
              </ul>
            </li>
            <li class="individual">
              <img src="images/team/David.jpg" alt="David Perich" />
              <h3>David Perich</h3>
              <p>Melbourne, AU</p>
              <ul class="social-icons">
                <li>
                  <a
                    href="mailto:davidnperich@gmail.com?subject=Venice%20Project"
                  >
                    <img src="images/icons/email_icon.png" alt="email" />
                  </a>
                </li>
                <li>
                  <a href="http://www.davidperich.com" target="_blank">
                    <img src="images/icons/website_icon.png" alt="website" />
                  </a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/davidperich/"
                    target="_blank"
                  >
                    <img src="images/icons/linked_in_icon.png" alt="linkedin" />
                  </a>
                </li>
              </ul>
            </li>
            <li class="individual">
              <img src="images/team/Melissa.png" alt="Melissa Manousos" />
              <h3>Melissa Manousos</h3>
              <p>Los Angeles, CA</p>
              <ul class="social-icons">
                <li>
                  <a
                    href="mailto:me@melissamanousos.com?subject=Venice%20Project"
                    target="_blank"
                  >
                    <img src="images/icons/email_icon.png" alt="email" />
                  </a>
                </li>
                <li>
                  <a href="https://melissamanousos.com/" target="_blank">
                    <img src="images/icons/website_icon.png" alt="website" />
                  </a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/melissa-manousos"
                    target="_blank"
                  >
                    <img src="images/icons/linked_in_icon.png" alt="linkedin" />
                  </a>
                </li>
              </ul>
            </li>
          </ul>
        </section>
      </div>
      <div class="design-credit">
        Logo and color scheme:
        <a href="http://linzimurray.creative" target="_blank"
          >linzimurray.creative</a
        >
        • Layout: <a href="http://jkulton.com" target="_blank">Jon Kulton</a>
      </div>
    </footer>
  </body>
</html>
