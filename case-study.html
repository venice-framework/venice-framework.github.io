<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="UTF-8" />
    <title>Venice</title>

    <link rel="stylesheet" href="stylesheets/reset.css" />
    <link rel="stylesheet" href="stylesheets/main.css" />

    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="images/icons/favicons/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="images/icons/favicons/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="images/icons/favicons/favicon-16x16.png"
    />
    <link rel="manifest" href="images/icons/favicons/manifest.json" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      href="https://fonts.googleapis.com/css2?family=Barlow+Condensed:wght@600&family=Fira+Code&family=Open+Sans:ital,wght@0,400;0,600;1,400&display=swap"
      rel="stylesheet"
    />
    <meta property="og:title" content="Venice Case Study" />
    <meta
      property="og:description"
      content="An open-source framework for deploying and managing stream processing pipelines"
    />
    <meta property="og:image" content="/images/thumbnail.png" />
    <meta name="theme-color" content="#0f4c75" />
    <script src="javascripts/sidebar.js"></script>
  </head>

  <body>
    <header class="header-short">
      <nav>
        <ul>
          <li>
            <a href="index.html">
              <img src="images/logos/venice_logo.png" alt="Venice logo" />
            </a>
          </li>
          <li><a href="case-study.html">Case Study</a></li>
          <li><a href="docs.html">Docs</a></li>
          <li><a href="team.html">Team</a></li>
          <li class="flex-float-right">
            <a href="https://github.com/venice-framework" target="_blank">
              <img
                src="images/logos/github-mark-light.png"
                alt="Venice GitHub"
                class="github"
              />
            </a>
          </li>
        </ul>
      </nav>
    </header>

    <div class="study-wrapper">
      <aside class="sidebar">
        <ul>
          <li>
            <a href="#Introduction"> 1. Introduction</a>
          </li>
          <li>
            <a href="#Background"> 2. Background</a>
          </li>
          <li>
            <a href="#Venice"> 3. Venice</a>
          </li>
          <li>
            <a href="#Architecture"> 4. Architecture & Deployment</a>
          </li>
          <li>
            <ul>
              <li>
                <a href="#Docker">
                  4.1 Docker
                </a>
              </li>
              <li>
                <a href="#Kafka-UI">
                  4.2 Kafka UI - Kafdrop
                </a>
              </li>
              <li>
                <a href="#Venice-CLI">
                  4.3 Venice CLI
                </a>
              </li>
            </ul>
          </li>
          <li>
            <a href="#Implementation"> 5. Implementation</a>
          </li>
          <li>
            <ul>
              <li>
                <a href="#Kafka">
                  5.1 Kafka
                </a>
              </li>
              <li>
                <a href="#ksqlDB">
                  5.2 ksqlDB
                </a>
              </li>
              <li>
                <a href="#Kafka-Connect">
                  5.3 Kafka Connect + PostgeSQL
                </a>
              </li>
              <li>
                <a href="#Schema-Registry">
                  5.4 Schema Registry
                </a>
              </li>
            </ul>
          </li>
          <li>
            <a href="#Challenges"> 6. Challenges</a>
          </li>
          <li>
            <ul>
              <li>
                <a href="#Data-Persistence"> 6.1 Data Persistence</a>
              </li>
              <li>
                <a href="#Containers"> 6.2 Containers</a>
              </li>
              <li>
                <a href="#Serialization-Format">
                  6.3 Data Serialization Formats</a
                >
              </li>
            </ul>
          </li>
          <li>
            <a href="#Future-Work"> 7. Conclusion & Future Work</a>
          </li>
          <li>
            <a href="#References"> 8. References</a>
          </li>
        </ul>
      </aside>

      <main>
        <section id="case-study">
          <h1>Case Study</h1>

          <h2 id="Introduction" class="sec-header">1. Introduction</h2>

          <p>
            One of the main challenges for event-driven microservices architecture is how the system handles the volume and velocity of data that needs to be distributed to different services. Many services may rely on the same event data but require it in different formats. One solution, event-stream processing, involves processing and transforming events as they arrive in the system before distributing them to their respective services. Event-stream processing decouples data writes from data reads. This means the ability to choose the ideal tools and formats for both reads and writes. Event-stream processing is a powerful solution for delivering event data in real time by centralizing event transformations in a single pipeline. 
          </p>

          <p>
            The implementation of these pipelines differs greatly, but
            essentially they have several components that are connected to
            achieve the following workflow:
          </p>

          <ol>
            <li>Event producers generate event messages</li>
            <li>
              The events are transmitted from the producer to the message broker
            </li>
            <li>
              From the broker, they are consumed by the stream-processing engine
              for transformation
            </li>
            <li>Event consumers read the transformed event data</li>
          </ol>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Pipeline-Generalized.png"
              alt="data source feeds message broker feeds stream processing engine feeds data sink"
            />
            <figcaption>
              Figure 1. A high-level overview of a generalized event-stream.
              processing pipeline
            </figcaption>
          </figure>

          <p>
            Developing and deploying an event-stream processing pipeline is
            typically expensive, complex, or both. Venice is an open-source
            framework that enables small teams starting out with event-stream
            processing to quickly deploy and manage an event-stream processing
            pipeline.
          </p>
          <p>
            Venice is built for smaller applications and for developers with
            limited event-streaming knowledge. It uses open source components to
            set up a stream processing pipeline with reasonable default settings
            and simple management tools within minutes.
          </p>
          <p>
            Developers building their first event-stream processing applications
            face some key challenges:
          </p>

          <ol>
            <li>
              Choosing between a number of potential components and creating a
              functioning pipeline.
            </li>
            <li>
              Connecting services to the pipeline to read the processed data.
            </li>
            <li>
              Ensuring consistency and correctness of data at all stages of the
              pipeline.
            </li>
            <li>
              Persisting event data and transformed data to populate
              materialized views and add new sinks as requirements change.
            </li>
          </ol>
          <p>
            Venice abstracts away much of this complexity so that developers can
            focus on application code, rather than building and future-proofing
            this pipeline. Venice is a fully extensible foundation from which
            small streaming applications can grow as requirements shift.
          </p>

          <p>
            This case study outlines the design, architecture, and
            implementation challenges of Venice. The value of Venice may become
            more apparent with insight into some of the problems that events and
            event-stream processing solve for distributed web applications.
          </p>

          <h2 id="Background" class="sec-header">2. Background</h2>

          <p>
            Many applications start out as simple monoliths. Over time, as
            requirements grow and become more complex, the need for scale
            prompts many to shift to a service-oriented architecture, such as
            microservices [<a href="#microservices" id="ref1">1, Ch. 1</a>].
          </p>

          <p>
            One of the goals of microservices is to decouple services from each
            other. However, multiple services often require access to the same
            underlying data, including historical data.
          </p>

          <p>
            How might an organization
            <strong>propagate data to multiple services in real-time</strong> without
            sacrificing <strong>availability</strong><sup>1</sup> or <strong>consistency</strong>,
            while retaining the flexibility to <strong>use historical data</strong>?
          </p>

          <p class="note">
            <sup>1</sup>Availability here means the opposite of downtime. If a
            system is responsive it is available.
          </p>

          <h3>
            2.1 It is difficult to share data between distributed services
          </h3>

          <p>
            An application might resemble this: a jumble of requests and data
            propagations between the application, a primary database, data
            warehouse, business analytics engine, search index, cache, and graph
            database.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/PrimaryDatabaseDependency.png"
              alt="A web application sends data to a primary database that feeds multiple other databases and processes"
            />
            <figcaption>
              Figure 2. The same data is needed for many different purposes.
              <sup>1</sup>
            </figcaption>
          </figure>

          <p class="note">
            <sup>1</sup> Figure inspired by [<a href="#mssp" id="ref2">2, Ch 2</a>].
          </p>

          <p>
            There are numerous ways to use data from this application, and
            different tools are better suited for different situations. For
            example, an e-commerce application might use orders data to
            continuously update:
          </p>
          <ul>
            <li>
              A cache of “deals of the day” and remaining inventory, to avoid
              overloading the database with redundant queries.
            </li>
            <li>
              A search index, to enable full-text search of available products.
            </li>
            <li>
              A key-value store of personalised product recommendations for
              customers.
            </li>
          </ul>

          <p>
            These outputs are all <strong>derived views</strong> of the same underlying
            data. Typically, it is difficult to maintain real-time services –
            such as the stock inventory, search index, or product recommendation
            services from above –
            <b
              >without sacrificing the availability of the services or
              consistency of data</b
            >. When a product runs out of stock, this change would ideally be
            propagated instantly to all these services, instead of hourly or
            daily. From a user and business perspective, it is desirable to
            update the shop as close to real time as possible. This would
            prevent displaying out-of-stock products to users.
          </p>

          <p>
            Dual writes are one way to update data across multiple services.
            However, partial failures, such as a network outage, can result in
            permanent inconsistency. Imagine writing orders to one database and
            updating remaining inventory in a different database. If a new order
            comes in for Product X, and is written successfully to the orders
            database, but the network crashes before the remaining inventory is
            updated, the two data stores are now inconsistent, and will remain
            that way without a manual recount.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/DualWriteFailure.png"
              style="max-height: 400px;"
              alt="A customer order is written to one database but the accompanying inventory update fails"
            />
            <figcaption>
              Figure 3. A successful write to one database and a failed write to the
              second.
            </figcaption>
          </figure>

          <p>
            Distributed transactions can provide consistency, but may sacrifice
            availability, because partial failures can render the application
            extremely slow or unresponsive [<a href="mssp" id="ref3">2, Ch. 2</a>;
            <a href="two-phase-commit">3</a>].
          </p>

          <p>
            To address this challenge, some companies have turned to using
            events to propagate data changes between their services.
          </p>

          <h3>2.2 Events drive application behavior</h3>

          <p>
            An event is an immutable object that describes something that
            happened at some point in time in an application [<a href="#ddia" id="ref4"
              >4, Ch. 11</a
            >]. For example, if a new order is placed, that order can be modeled
            as an event as follows:
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/OrderEventObject.png"
              style="max-height: 400px;"
              alt="order event object with properties for order_id, customer_id, seller_id, product_id, quantity, price_in_cents, timestamp"
            />
            <figcaption>Figure 4. An example of an order event.</figcaption>
          </figure>

          <p>
            An event can trigger one or more actions. For example, when a new
            order comes in, the event can trigger an update to the inventory,
            start the fulfillment process, and send an email notification to the
            customer.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/OrderTriggersServices.png"
              style="max-height: 400px;"
              alt="data from the order object in the previous figure is sent to three different services"
            />
            <figcaption>
              Figure 5. A new order triggers the inventory management,
              fulfillment, and notifications services.
            </figcaption>
          </figure>

          <p>
            In streaming terminology, a producer generates an event. Related
            events are grouped together into a stream (also known as a topic).
            That event is then processed by one or many consumers [<a
              href="#ddia" id="ref5"
              >4, Ch. 11</a
            >]. In the example above, the application is the producer. When a
            customer creates a new order, the application produces an event to
            the orders stream. The inventory management, fulfillment and
            notifications services are the consumers who receive the event and
            take appropriate action.
          </p>

          <h3>2.3 Message brokers move data from producers to consumers</h3>

          <p>Message brokers are suited for situations where:</p>

          <ul>
            <li>producers and consumers are asynchronous.</li>
            <li>
              multiple producers may write to a topic, and multiple consumers
              may read from a topic.
            </li>
          </ul>

          <p>
            There are two common types of message brokers: (1) message queues
            (brokers that implement the AMQP and JMS standards) and (2)
            log-based message brokers.
          </p>

          <h4>
            2.3.1 Message queues do not retain events and may process events out
            of order
          </h4>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/gifs/MessageQueue.gif"
              style="max-height: 400px;"
              alt="gif of a message queue with messages enqueing and dequeuing"
            />
            <figcaption>
              Figure 6. A message queue: new messages enqueue and old messages
              dequeue.
            </figcaption>
          </figure>

          <p>
            Message queues (e.g., RabbitMQ, ActiveMQ) are preferable in
            situations where:
          </p>

          <ul>
            <li>each event may take a long time to process.</li>
            <li>processing order is not important.</li>
            <li>event retention is not required.</li>
          </ul>

          <p>
            Recall that the motivating question was: How might an organization
            propagate data to multiple services in real-time without sacrificing
            availability or consistency, while retaining the flexibility to use
            historical data?
          </p>

          <p>
            Message queues are not the answer because they may process events
            out of order and do not retain events [<a href="#ddia" id="ref6">4, Ch. 11</a
            >].
          </p>

          <h4>
            2.3.2 Log-based message brokers retain events and help guarantee
            order
          </h4>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Log.png"
              style="max-height: 400px;"
              alt="a row of numbers in boxes, each box represents a message written to the log"
            />
            <figcaption>
              Figure 7. A log-based message broker: new messages are appended to
              an immutable log.
            </figcaption>
          </figure>

          <p>
            Log-based message brokers (e.g., Apache Kafka, Amazon Kinesis
            Streams) are preferable in situations where:
          </p>

          <ul>
            <li>each event can be processed quickly.</li>
            <li>processing order is important.</li>
            <li>
              event retention is required [<a href="#mssp" id="ref7">2, Ch. 2</a>;
              <a href="#ddia">4, Ch. 11</a>].
            </li>
          </ul>

          <p>
            Log-based message brokers are the answer to the motivating question.
          </p>

          <ol>
            <li>
              <strong>Data can be propagated to services in real-time.</strong><br />
              Logs support low latency and high throughput writes and reads.<br />
              <ul>
                <li>
                  Low Latency: Each write is an append operation, and a read is
                  a linear scan. Producers can write data to the log in a single
                  step, and consumers can read data from the log very quickly.
                </li>
                <li>
                  High Throughput: Logs can be split into partitions and
                  replicated across machines. This means many reads and writes
                  can proceed concurrently [<a href="#okay-store-data" id="ref8">5</a>].
                </li>
              </ul>
            </li>
            <li>
              <strong>The system is fault-tolerant.</strong>
              <ul>
                <li>
                  With multiple brokers, messages are rerouted to replicas if
                  one crashes.
                </li>
                <li>
                  With partitioned data and load-balanced consumers, if one
                  consumer fails, others can pick up the work.
                </li>
              </ul>
            </li>
            <li>
              <strong>Data is consistent across services.</strong>
              <ul>
                <li>
                  Events are ordered within partitions. Keyed partitions route
                  events with the same key to the same partition, ensuring
                  order. For example, partitioning by product ID allows all
                  orders for Product X to be processed in the order they are
                  produced.
                </li>
                <li>
                  Logs are immutable. Services that require the same underlying
                  data will all see the same data. This avoids inconsistencies
                  due to race conditions associated with having multiple
                  databases.
                </li>
              </ul>
            </li>
            <li>
              <strong>Historical data is preserved.</strong>
              <ul>
                <li>
                  Logs contain historical information and new information. New
                  services can be added to the system at any time, consume the
                  historical data, and follow future events automatically.
                </li>
                <li>
                  Developing new services is also possible, with the option to
                  gradually transition, or roll back changes.
                </li>
              </ul>
            </li>
          </ol>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/LogAsBroker.png"
              style="max-height: 400px;"
              alt="generic data sources send data to a log-based message broker that feeds into a data sink"
            />
            <figcaption>
              Figure 8. Event producers send messages to a partitioned log-based
              broker. Event consumers use these event messages in various ways.
            </figcaption>
          </figure>

          <h4>2.3.3 A note on the limitations of logs</h4>
          <p>
            Log-based approaches are not the answer to all problems. Events are
            consumed in a linear fashion, so a single event taking a long time
            to process could delay processing of later events. [<a href="#ddia" id="ref9"
              >4, Ch. 11</a
            >]. Ordering of events is only guaranteed within an individual
            partition, which may be problematic as the system scales. The system
            also does not support reading self writes [<a href="#ddia"
              >4, Ch. 5</a
            >], and other linearizability guarantees [<a href="#ddia"
              >4, Ch.9, Ch. 12</a
            >]. In many cases, the benefits outweigh the drawbacks, and
            log-based message brokers present a powerful solution to the problem
            of <strong>propagating data to multiple services in real-time</strong>,
            without sacrificing <strong>availability</strong> and <strong>consistency</strong>,
            while retaining the flexibility to <strong>use historical data</strong>.
          </p>

          <h3>
            2.4 Stream processors abstract common operations out to a fast and
            fault-tolerant engine
          </h3>

          <p>
            It is common to have multiple services use the same data to generate
            different outputs in an application. The outputs generated may be
            different, but many of the operations used by the services to
            process these events are the same. These operations include
            windowing, aggregation, joins, filters, and transformations [<a
              href="#streaming-systems" id="ref10"
              >6</a
            >].
          </p>

          <p>
            Individual consumers can be written to perform these operations.
            This works well for operations such as filtering or transforming
            that process one message at a time. This approach becomes more
            challenging when operations are more complex, for example,
            aggregating over time or joining two different streams together.
          </p>

          <p>
            Event stream processors abstract these common processing operations
            to an engine. Such an engine can provide additional benefits such as
            fault tolerance, efficient processing of events via clusters,
            maintenance of local state, and out-of-order events handling [<a
              href="kafka-streams" id="ref11"
              >7</a
            >].
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/gifs/ProcessorReplacesConsumers.gif"
              style="max-height: 400px;"
              alt="gif of several consumers being replaced by a single stream processing engine"
            />
            <figcaption>
              Figure 9. An event-stream processing engine replaces the need for
              individual consumers performing redundant processing tasks.
            </figcaption>
          </figure>

          <p>
            Adding a stream processor to a log-based architecture expands the
            benefits of the system. Services can write data efficiently by
            appending it to the log. The stream processor can complete
            operations shared by different services more efficiently. This means
            that services can share the same underlying data and easily
            transform that data into a format that is ideal for their needs.
          </p>

          <h4>2.4.1 A note on the importance of databases</h4>

          <p>
            This background emphasizes the benefits of an event-driven
            architecture with a distributed log at its core. However, event
            driven architecture does not replace databases completely.
          </p>

          <p>
            Databases prioritize state, while events prioritize state changes.
            State is derived from events – for example, your physical location
            as you read this is the result of the event that moved you there. In
            an application, both events and state are useful. Databases in an
            architecture provide fast access to state.
          </p>

          <p>
            A log-based, event-driven architecture utilizes distributed logs and
            databases to provide the benefits of both in a fault-tolerant and
            scalable way.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Brokers-Processor.png"
              style="max-height: 400px;"
              alt="producers or sources such as user activity, sensor data, or database writes feed events to a replicated log-based message broker which sends them to a stream processing engine and eventually they wind up in various consumers or sinks such as a cache, graph database, or search index"
            />
            <figcaption>
              Figure 10. An example of a log-based stream processing
              architecture.
            </figcaption>
          </figure>

          <h3>2.5 Summary</h3>

          <ul>
            <li>
              In a microservices architecture, many services
              <strong>require access to the same data</strong>.
            </li>
            <li>
              Propagating data to many services is difficult with architecture
              centered on <strong>databases</strong>.
            </li>
            <li>
              <strong>Events</strong> can help address this problem. Events represent what
              happened in the application and when.
            </li>
            <li>
              <strong>Message brokers</strong> help move events between
              <strong>producers</strong> (data creators, such as an application) and
              <strong>consumers</strong> (services that need the data).
            </li>
            <li>
              <strong>Log-based message brokers</strong> can propagate data to services in
              real time in a fault-tolerant, consistent, and scalable way, and
              provide the flexibility to use historical data.
            </li>
            <li>
              <strong>Event stream processors</strong> abstract out common operations on
              event data to a fault-tolerant and scalable engine that maintains
              event order.
            </li>
            <li>
              An architecture with a distributed event log at its core and
              databases to store outputs provides
              <strong>fast access to both events and state</strong>. Multiple services can
              now use the same data in different ways, and the application has
              the agility to grow with changing requirements.
            </li>
          </ul>

          <h2 id="Venice" class="sec-header">
            3. Venice
          </h2>

          <p>
            Venice is an open-source framework that enables developers starting
            out with event-stream processing to quickly deploy and manage an
            event-stream processing pipeline.
          </p>

          <p>
            Venice is built for smaller applications and for developers with
            limited event-streaming knowledge. It uses open source components to
            set up a stream processing pipeline with reasonable default settings
            and simple management tools within minutes.
          </p>

          <p>
            Large companies typically use stream processing to perform the
            following tasks:
          </p>

          <ul>
            <li>
              Process complex real-time event data to make applications more
              responsive.
            </li>
            <li>Detect anomalies in events.</li>
            <li>
              Deliver real-time analytics without interfering with the
              application’s ability to write new data.
            </li>
          </ul>

          <p>
            The desire to perform these tasks, however, is not unique to large
            enterprises with applications that process millions of events per
            day. Any growing application might benefit from event stream
            processing. The implementation of these pipelines differs, but they
            have several interconnected components that achieve the following
            workflow:
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Pipeline-Generalized.png"
              alt="one or more event processors (data sources) feed into a message broker (typically Kafka - a log) which feeds into a stream processing engine ending at event consumers also known as data sinks (visualization tools, caches, databases, Kafka topic, etc.)"
            />
            <figcaption>
              Figure 11. A generalized event-stream processing pipeline.
            </figcaption>
          </figure>

          <h3>3.1 Venice sits between DIY and managed solutions</h3>

          <p>
            The existing choices available to developers roughly fall into two
            categories: managed solutions and do-it-yourself (DIY).
          </p>

          <p>
            Managed solutions (such as Confluent’s Cloud Platform or Landoop)
            empower developers to focus on their applications by offering
            configuration guidance and on-going support for teams implementing
            streaming systems. However, they have high price tags, are generally
            designed for large enterprises, and can lock the team into using one
            vendor.
          </p>

          <p>
            The DIY approach is an option for teams newer to this space.
            However, it trades monetary cost for complexity. First, application
            developers must deal with an abundance of choice. For the stream
            processing engine alone, there are multiple open-source options and
            several paid ones [Figure 12].
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/streamProcessingEngines.png"
              alt="logos for AWS Kinesis, Google CloudData, and Apache products: Samza, Spark, Flink, Apex, Flume, Heron, Storm, Kafka Streams, Beam, ksqlDB"
            />
            <figcaption>
              Figure 12. A sampling of the options for stream processing
              engines.
            </figcaption>
          </figure>

          <p>
            Second, because the stream-processing space is emerging,
            configuration tutorials become quickly outdated and documentation is
            minimal or challenging to parse. Choosing and learning how to
            configure a single component in the pipeline could take days or
            weeks. The problem compounds when attempting to learn which
            components best integrate together to form a pipeline with tradeoffs
            that are acceptable for the project.
          </p>

          <p>
            Existing managed solutions and open source options may not be
            suitable for cost-conscious developers with limited streaming
            knowledge developing small streaming applications. Venice aims to be
            the middle ground - balancing the need for affordability, simplicity
            and control.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/VeniceVsDIY-Managed.png"
              alt="Three arrows representing cost, ease of use, and user control with Venice, DIY, and managaged solutions arranged along them accordingly"
            />
            <figcaption>
              Figure 13. Venice positions itself between DIY and managed
              solutions in terms of cost, ease of use, and user control.
            </figcaption>
          </figure>

          <h3>
            3.2 Design goals for an approachable stream processing framework
          </h3>

          <div id="design-goals">
            <p>
              Venice is approachable for developers new to event-stream
              processing.<br> It allows them to:
            </p>
            <ul>
              <li><img src="images/icons/clock.png" alt="clock" class="goals"/>Set up common pipeline patterns in minutes so
                they can minimize time spent on configuration and focus on
                application code instead.
              </li>
              
              <li><img src="images/icons/plus.png" alt="plus" class="goals"/>Retain extensibility and potential for
                reconfiguration if their needs change or become more complex.
              </li>
            </ul>
          </div>

          <h2 id="Architecture" class="sec-header">4. Architecture and Deployment</h2>

          <p>Venice consists of these components:</p>

          <ul>
            <li><strong>Kafka brokers</strong> for logging events</li>
            <li>A <strong>Zookeeper</strong> instance for managing the cluster of Kafka brokers</li>
            <li>A <strong>Schema Registry</strong> for managing schemas</li>
            <li>A <strong>ksqlDB server</strong> for performing aggregations, filtrations, transformations, and joins over real-time data</li>
            <li><strong>KSQL CLI</strong> for interacting with the ksqlDB server</li>
            <li><strong>Kafka Connect</strong> workers for moving data in and out of Kafka and simplifying the process of adding new sources and sinks</li>
            <li>A <strong>PostgreSQL</strong> data sink for storing the data in multiple formats</li>
            <li><strong>Kafdrop</strong> UI for monitoring message brokers</li>
            <li><strong>Venice CLI</strong> for deploying and managing the pipeline</li>
          </ul>

          <p>The components are packaged in Docker containers, making them easy to build and deploy or swap for others to better meet changing developer needs.</p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/VeniceFull.png"
              alt="Venice pipeline architecture diagram"
            />
            <figcaption>
              Figure 14. The complete Venice pipeline.
            </figcaption>
          </figure>

          <p>The workflow is as follows:</p>

          <ul>
            <li>The user installs and launches Venice through the Venice CLI.
              <figure class="img-wrapper">
                <img
                  src="images/diagrams/gifs/setup.gif"
                  alt="gif: 'venice install' allows a user to select which directory to clone from github; 'venice up' launches the pipeline."
                />
                <figcaption>
                  Figure 15. CLI commands <code>venice install</code> and <code>venice up</code> in action.
                </figcaption>
              </figure>
            </li>
<li>
  The user connects a producer which generates events and sends them to the Kafka Brokers, which are managed by Zookeeper. 
<ul>
  <li>
    Producers optionally register schemas with the Schema Registry. 
  </li>
  <li>Additional producers can be configured to connect to the pipeline through Kafka Connect.</li>
</ul>
</li>

<li>
  Kafka records the events in order and serializes them for transport.  
  <ul>
    <li>
      The user can verify the serialization, topics, and more using the Kafka UI or Venice CLI.
    </li>
  </ul>
</li>

<li>
  The stream processing engine performs queries as it receives each event and sends the transformed outputs back to Kafka to a new Kafka topic.
<ul>
  <li>The user has the option of launching and using the KSQL CLI through the Venice CLI to add new queries to the ksqlDB server.</li>
</ul>
</li>

<li>
  Consumers can read directly from the Kafka Brokers, retrieving schemas from the Schema Registry to deserialize data. 
</li>

<li>Kafka Connect connectors (PostgreSQL or additional optional user-added data sinks) synchronize data from Kafka to the database, automatically retrieving schemas from the schema registry.</li>
          </ul>

          <p>Each of the components in the <i>“Managed by Venice CLI”</i> box runs within a separate Docker container.</p>

          <h3 id="Docker" class="sec-header">4.1 Docker isolates pipeline components</h3>

          <p>The Venice pipeline contains multiple components and each has its own set of dependencies. These dependencies could conflict with each other or a user’s existing software. Docker isolates each component, preventing such dependency collisions. This provides streamlined installation as well as smoother addition of additional components. 
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/ContainerizedServices.png"
              alt="Three services with their dependencies isolated from one another in solid boxes inside a dotted box 'Managed by Docker' all within a larger 'Host Machine' box"
            />
            <figcaption>
              Figure 16. Docker isolates each service and its dependencies in containers.
            </figcaption>
          </figure>

          <h3 id="Kafka-UI" class="sec-header">4.2 Kafdrop provides visual insights to help manage stream processing pipelines</h3>

          <p>A source of complexity for a developer new to event stream processing is the difficulty of gaining insight into the system. For a developer new to stream processing  it is often arduous to: 
          </p>

          <ul>
            <li>Discover whether messages are being read and encoded properly or divided among partitions as desired. </li>
            <li>Schemas are registered or evolving as intended. </li>
            <li>Connectors are present and if their tasks are running or failed. </li>
            <li>Topics are available or created as requested. </li>
          </ul>

          <p>To this end, Venice includes the third-party Kafdrop graphic user interface. Once a Venice user deploys the pipeline, they can navigate to a simple web interface that grants them a window into all of these features and more. 
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/gifs/kafdrop_view_messages.gif"
              alt="gif of a user viewing bus event data in the Kafkdrop Kafka graphical user interface"
            />
            <figcaption>
              Figure 17. The Kafdrop Kafka cluster graphical user interface
            </figcaption>
          </figure>

          <h3 id="Venice-CLI" class="sec-header">4.3 Venice CLI streamlines developer experience</h3>

          <p>The commands to interact with a containerized event streaming system can be a source of frustration. Many of the commands to manage the Venice containerized components are unwieldy and require an intimate knowledge of container names, ports, etc. Venice provides a CLI with 14 commands to streamline these interactions for developers.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/CLI-Admin-Commands.png"
              alt="list of Venice CLI administrative commands"
            />
            <img
              src="images/diagrams/CLI-Connector-Topic-Commands.png"
              alt="list of Venice CLI connector and topic commands"
            />
            <figcaption>
              Figure 18. Commands for the Venice CLI.
            </figcaption>
          </figure>

          <h2 id="Implementation" class="sec-header">5. Venice Implementation</h2>

          <p>Venice collects open-source components into a basic pipeline that developers can use with minimal configuration. It is stripped down to reduce troubleshooting headaches but filled with enough tools and features to help developers new to the space ramp up quickly. Venice allows teams to:</p>

          <ul>
            <li>Connect their own producer(s) or build from the available Venice producer.</li>
            <li>Process streaming data using SQL-like syntax.</li>
            <li>Connect consumers to the pipeline to read the processed data. </li>
            <li>Ensure consistency and correctness of data at all stages of the pipeline. </li>
          </ul>

          <p>Each included component helps developers with these tasks.</p>

          <hr />

          <h3 id="Kafka" class="sec-header">5.1 Kafka is at the heart of the Venice pipeline</h3>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Pipeline-Kafka.png"
              alt="A cluster of Kafka brokers managed by Zookeeper replaces the generic log from the generic pipeline diagram"
            />
            <figcaption>
              Figure 19. A cluster of log-based Kafka brokers (managed by Zookeeper) receive event data from the user’s producer(s).
            </figcaption>
          </figure>

          <p>Kafka is a message broker built on a distributed append-only log and is by far the most popular open-source log-based message broker. It has a robust ecosystem, good documentation, and active development. </p>

<p>Venice has a three-broker cluster with partitions distributed across the brokers to enable higher throughput and fault tolerance via redundancy. This cluster is managed by an instance of Apache Zookeeper which handles tasks like leader elections.</p>

<p>Kafka is part of an event streaming ecosystem that includes some additional components used in Venice’s default pipeline: </p>

  <ul>
<li><b>ksqlDB</b>: an event streaming database that Venice uses primarily as an approachable stream processor. </li>
<li><b>Kafka Connect</b>: a framework for automatically copying data between Kafka and external systems.</li>
<li><b>Confluent Schema Registry</b>: a tool to validate and manage schemas.</li>
  </ul>

  <p>The importance of these components is addressed in turn.</p>

  <hr />

<h3 id="ksqlDB" class="sec-header">5.2 ksqlDB: a stream processor with SQL-like syntax</h3>

<figure class="img-wrapper">
  <img
    src="images/diagrams/Pipeline-ksqlDB.png"
    alt="ksqlDB replaces the stream processor from the generic pipeline diagram"
  />
  <figcaption>
    Figure 20. ksqlDB (and its optional CLI) provides an approachable stream-processing engine.
  </figcaption>
</figure>

<p>Event stream processing is complex. For developers new to the domain, there is a lot to learn. Venice keeps things as simple as possible. Many stream processing engines (Apache Spark, Apache Flink, Apache Kafka Streams) require developers to write event-stream processing tasks in Java or Scala. This presents a barrier to entry for developers unfamiliar with these languages. Additionally, several Java stream processing engines (Spark, Flink) require dependencies for Java JDK and Apache Maven. These introduce additional complexity that might make it more challenging for the Venice user to troubleshoot problems in the pipeline. </p>

<p>Venice implements ksqlDB as the primary stream processor. ksqlDB is not technically a stream processing framework, but an abstraction over the Kafka Streams stream processing library. ksqlDB provides much of the functionality of the more robust engines while allowing developers to use the declarative SQL-like syntax seen in Figure 21. </p>

<figure class="img-wrapper">
  <img
    src="images/diagrams/SQL-KSQL.png"
    alt="code samples of SQL and KSQL"
  />
  <figcaption>
    Figure 21. An example for both SQL and KSQL filtering orders from platinum customers. TheSQL query provides a snapshot of these orders for a moment in time. The KSQL STREAM, is a standing query that will continuously update as new orders arrive.  
  </figcaption>
</figure>

<p>ksqlDB, with its support for a SQL-like syntax, allows developers to learn how to write streaming applications more quickly. ksqlDB has a CLI that Venice includes as an optional component. It is not available within the base Venice Docker pipeline but is easily accessible via the CLI.</p>

<p>It should be noted that Java- or Scala-fluent developers requiring higher complexity in their stream processing applications can still use the base Venice pipeline with Kafka Streams, which is simply a Kafka client library readily available to the system [<a href="#streams-vs-ksql" id="ref12">8</a>]. Developers with differing skill sets can use the pipeline in various ways to maximize their experience. </p>

<hr />

<h3 id="Kafka-Connect" class="sec-header">5.3 Connecting Consumers: Kafka Connect and PostgreSQL</h3>

<figure class="img-wrapper">
  <img
    src="images/diagrams/Pipeline-KafkaConnect+Postgres.png"
    alt="Kafka Connect builds into the generalized pipeline to add a PostgreSQL database and optionally links to the generic Event Consumer(s)"
  />
  <figcaption>
    Figure 22. Kafka Connect simplifies adding data sinks and sources; PostgreSQL provides developers a familiar database for their transformed data.
  </figcaption>
</figure>

<p>In a streaming application there are common patterns to consume data.</p>

<ol>
  <li>Consume events directly from Kafka to perform real-time actions. </li>
<li>Consume events automatically to keep a data store in sync with specific topic(s) on Kafka. </li>
</ol>

<p>Venice is configured to enable both patterns as an application will often require a combination.</p>

<h4>Pattern 1. Connect to Kafka Broker and Schema Registry directly.</h4>

<p>ksqlDB writes the results of transformations back to Kafka as a new topic.  As user can write consumer applications in the language of their choice<sup>1</sup> that respond in real-time to the results of stream processing. For example, a user can filter high priority events into a new topic and have an application consume that topic. </p>

<p class="note"><sup>1</sup> There is the requirement that the chosen language can connect to Kafka and the Schema Registry and deserialize Avro. This capability is available in most popular languages [<a href="#kafka-clients" id="ref13">9</a>].</p>

<h4>Pattern 2. Use Kafka Connect to automatically send events to a data sink.</h4>

<p>Kafka Connect is a framework for copying data between systems. Kafka Connect serves as a centralised hub for connectors that synchronize data between Kafka and various data stores. Sink connectors can be considered consumers - configured instead of written - to synchronize data between Kafka and a datastore. This is useful for: </p>

<ul>
<li>Ad-hoc queries that are too infrequent to run as stream processing jobs, but still benefit from up-to date data. </li>
<li>Automatically syncing data to data stores like databases or caches.</li>
</ul>

<p>Venice uses Kafka Connect in the default pipeline to connect to a PostgreSQL database. A user can sink any topic on Kafka to a new table in Postgres via the Venice CLI. The table will be updated as new events arrive in Kafka. Users can then perform ad-hoc queries on this database and know that the results will be in sync with the rest of their pipeline. </p>

<p><a href="https://www.confluent.io/hub/" target="_blank">Confluent Hub</a> hosts hundreds of Connectors for popular data sources and sinks that can extend the pipeline<sup>1</sup>.</p>

<p>Consuming real-time data from the pipeline is the overarching goal of an event-stream processing pipeline. Kafka Connect and PostgreSQL bolster the functionality of ksqlDB to ensure Venice meets the most common consumer patterns for a streaming application.</p>

<p class="note"><sup>1</sup> Adding additional connectors requires configuration of connectors, tasks, and convertors.</p>

<hr />

<h3 id="Schema-Registry" class="sec-header">5.4 Confluent Schema Registry and Avro data format help provide data consistency and correctness for consumers</h3>

<figure class="img-wrapper">
  <img
    src="images/diagrams/Pipeline-SchemaRegistry.png"
    alt="the Schema Registry connects to the Kafka brokers, Kafka Connect, and the ksqlDB - sharing schemas throughout the pipeline"
  />
  <figcaption>
    Figure 23. The Schema Registry assists with data consistency and correctness.
  </figcaption>
</figure>

<p>Kafka, ksqlDB, and Kafka Connect provide the previously-discussed  functionality: ingest and store in the log, manipulate in the stream processor, and connect to output in the sink. Venice also includes the Schema Registry to help the entire system operate more smoothly.</p>
 
<p>Kafka does not enforce any particular format for event data beyond a simple key/value pair. Kafka is not aware of the data format as all records are stored as byte arrays. While Kafka doesn’t care about the data format, consumers do. Attempting to deserialize an event from byte array to the wrong data format is one of the most common problems when writing new consumers. </p>

<h4>5.4.1 Confluent Schema Registry</h4>
<p>The simplicity of Kafka comes from having all the data in one place and having all the data follow similar conventions. Venice includes the Confluent Schema Registry and encourages developers to use the open source Avro data format. These two tools bolster the benefits of Kafka by supporting data consistency and minimizing the data transfer burden on the network. </p>

<p>The Schema Registry exists apart from the Kafka brokers. Producers and consumers interact directly with Kafka to write and read data and concurrently communicate with the Schema Registry [Figure 24] to send and retrieve schemas.</p>

<figure class="img-wrapper">
  <img
    src="images/diagrams/SchemaRegistry.png"
    alt="Producer feeds feeds data to Kafka; schema to the registry. Consumer pulls data from Kafka; schema from registry"
  />
  <figcaption>
    Figure 24. Producers write events to Kafka while submitting the event schema to the Schema Registry. Consumers retrieve the schema from the Registry when they read the event from the Kafka broker.
  </figcaption>
</figure>

<p>A schema registry provides three main benefits: </p>

<ul>
<li><strong>Schemas are stored in one place.</strong> Any service requiring the data in a topic can request the schema.</li>
<li><strong>Schema changes are tracked.</strong> Tracking schema changes and when they occurred allows users to interpret and use all of the data.  </li>
<li><strong>Schema validations can be performed.</strong> Data that does not conform to the schema can be rejected from the log. This prevents typos or incorrect data types from being written to the log. </li>
</ul>

<h4>5.4.2 Avro Data Format</h4>

<p>Venice recommends users format their data using Avro [Figure 25]. Avro provides a rich schema definition language, integrates with the Confluent Schema Registry, and is sent efficiently over the network in a compact binary format [<a href="#avro" id="ref14">10</a>]. </p>

<figure class="img-wrapper">
  <img
    src="images/diagrams/Avro.png"
    alt="Two JSON objects: one describing event data; one the schema dictating the data type of each property in the object."
  />
  <figcaption>
    Figure 25. An Avro event data object and accompanying schema - both are written in JSON, but translated to a more compact binary format when sent over the network.
  </figcaption>
</figure>

<p>The Schema Registry doesn’t eliminate issues of consistency and correctness. That depends on how Venice users develop event producers and event-streaming queries. However, with this component users have a tool to reduce the risk of introducing incorrect or incompatible data in their pipeline. </p>

<p>The above four components - Kafka, ksqlDB, Kafka Connect, and the Schema Registry - provide the foundation of the Venice pipeline. They help make Venice flexible, extensible, and robust. </p>


<h2 id="Challenges" class="sec-header">6. Challenges</h2>

          <p>
            Venice solves for several challenges: persisting data, reliably
            launching and connecting component containers, and consistently
            serializing data.
          </p>

          <h3 id="Data-Persistence">
            6.1 How can Venice persist data to make materialized views and new
            services possible?
          </h3>
          <p>
            This challenge has three parts: data persistence, storage choice,
            and log retention strategy.
          </p>

          <h4>6.1.1 How should Venice persist data?</h4>

          <p>
            Retaining events indefinitely in Kafka allows users to build new
            services or re-design existing services that can consume the entire
            history of events to recreate application state. This is a critical
            feature for teams new to streaming applications who will be
            experimenting to discover what works.
          </p>

          Docker provides two ways to persist data for containers -
          <b>volumes</b> and <b>bind mounts</b>.

          <figure class="img-wrapper">
            <img
              src="images/diagrams/DockerVolumes_bindMounts.png"
              alt="An arrow points from the Docker container to the Docker area within the host machine created by a volume. A second arrow points to host machine in general to represent a bind mount."
            />
            <figcaption>
              Figure 26. Docker containers interact with the host machine’s file
              system through bind mounts or volumes.
            </figcaption>
          </figure>

          <p>
            The main difference, shown in Figure 26, is where data is saved on
            the host machine. With volumes, Docker manages storing data in a
            restricted Docker directory. With a bind mount, a specific path on
            the host machine is loaded on the container at startup.
          </p>

          <p>
            Venice uses bind mounts to ensure Docker loads a specific path from
            the host machine into the container everytime a container starts.
            When using volumes, new volumes are created when containers are
            re-configured and rebuilt. Bind mounts are a more reliable and
            consistent way of reloading data into the container for developers
            experimenting with new pipelines.
          </p>

          <h4>
            6.1.2 What is the optimal way for Venice to provide event
            persistence?
          </h4>

          <p>
            There are three options for persisting Kafka events indefinitely:
            permanent log retention, external backups, and compacted topics.
          </p>

          <h5>Choice 1: Permanent Log Retention</h5>

          <p>
            Typically, Kafka will delete events after a predetermined amount of
            time (e.g., 7 days) or when the log reaches a certain byte size.
            However, permanently preserving events is possible by overriding the
            default retention settings. A new consumer could then consume
            everything from the beginning of the log to derive the current
            state.
          </p>

          <h5>Choice 2: External Backup</h5>

          <p>
            Another option is to periodically copy older events to an external
            data store and remove them from Kafka. To recreate state for a new
            consumer, the backed up data must be reloaded and configured in
            Kafka.
          </p>

          <h5>Choice 3: Compacted Topics</h5>

          <p>
            Finally, log compaction refers to retaining the last known value for
            each message <b>key</b> within a single topic partition. Compaction
            guarantees event order by only removing older events whose state has
            changed.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/TopicCompaction.png"
              alt="Two event objects. One has the email adddress 'daviddd@gmail.com'; the other, 'david@gmail.com'. A box around the second indicates it will be preserved."
            />
            <figcaption>
              Figure 27. An example of a user updating their email address. With
              a compacted topic only the latest value related to the
              <code>user_id: 1</code> is retained.
            </figcaption>
          </figure>

          <p>
            For example, Figure 27 shows events that track a change to a user’s
            email, where the user_id is the key. With a compacted topic, Kafka
            would guarantee that the most recent update to the email would be
            kept in the log.
          </p>

          <p>
            Log compaction can be suitable for some topics. However, it means
            losing the ability to <strong>recreate</strong> state, leaving only the option
            to <strong>restore</strong> the latest state.
          </p>

          <h5>Venice: Permanent Log Retention</h5>

          <p>
            Venice uses permanent log retention because (1) it is a simpler
            solution than external backup, and (2) log compaction does not not
            fulfill the goal of extensibility. A compacted topic means users
            would not be able to consume historical data.
          </p>

          <p>
            Permanent log retention is a safe option: users have all of their
            events stored by default, but they can choose to turn on log
            compaction for individual topics if they decide that is appropriate
            for their use case.
          </p>

          <p>
            However, as a user’s application grows and events accumulate, they
            may want to consider external backup to free up space on the
            machines running the brokers.
          </p>

          <h3 id="Containers" class="sec-header">6.2 How can Venice reliably launch and connect containers?</h3>

          <p>Each component in the Venice pipeline launches in its own container. This poses a few challenges. 
          </p>

          <h4>6.2.1 How can Venice launch all of the containers with a single command?</h4>

          <p>A user would likely want to experiment with their pipeline, which means they would be starting and stopping the entire pipeline multiple times. Docker Compose allows Venice to define all of the pipeline components in a single YAML file and specify the startup order. A single command would start and stop all of the containers.</p>
          
          <h4>6.2.2 How can containers securely and reliably communicate with each other? </h4>

          <p>Docker Compose automatically creates a network for all components defined in the YAML file. This network enables communication between containers while isolating them from the host network. However, the name of this automatically generated network is dependent on the name of the directory where the YAML file is located[<a href="#docker-compose" id="ref15">11</a>, <a href="#compose-networking">12</a>]. For example, if the directory was called “venice”, the name of the default network would be “venice_default”. </p>
          <p>The Venice CLI needs to work with components on the network regardless of the names of the directories, so it uses Docker Compose’s custom networks feature to define a network named “venice”.</p>

          <h4>6.2.3 How can Venice address race conditions between containers?</h4>

          <p>Venice can dictate the startup order for each container, but sometimes this is not enough [<a href="#compose-startup" id="ref16">13</a>]. Just because a container is running does not mean it is listening on a port or ready to send or receive messages.</p>

          <p>Two problems stemming from this are: </p>

          <ol>
            <li>Producers crash if they attempt to connect to the Schema Registry before it is ready.</li>
            <li>The Venice <code>connector-init</code> service fails to initialize stored connectors on startup if Kafka Connect is not ready.</li>
          </ol>

          <p>
            Venice solves these dependency issues with shell scripts that ping for confirmation that the Schema Registry and Kafka Connect are ready to receive connections before running producers or initializing connectors [<a href="#rmoff-tricks" id="ref17">14</a>].
          </p>

          <h3 id="Serialization-Format">6.3 What default serialization format should Venice recommend?</h3>
          <p>Venice encourages developers to use <a href="https://avro.apache.org/docs/current/" target="_blank">Avro</a> but Avro keys are incompatible with ksqlDB.
          </p>

          <p>In Kafka, each event key and value is written as a pair of raw bytes. If a producer serializes the event in one format and the consumer deserializes it in another, the application will produce errors or the data will be unusable [Figure 28]. 
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/UnusableData.png"
              alt="screen capture of the Kafdrop UI showing message key and value as empty squares in place of characters"
            />
            <figcaption>
              Figure 28. Incorrect deserialization format causes unreadable key and value on Kafdrop.
            </figcaption>
          </figure>

          <p>ksqlDB requires <code>STRING</code> event keys, but the default configuration for many connectors is Avro keys and values. Encoding the key as a <code>STRING</code> and the value in Avro allows users to leverage all of ksqlDB’s functionalities while retaining many of the benefits of using Avro (since values typically contain more data than keys). </p>

          <h2 id="Future-Work" class="sec-header">7. Future Work</h2>
          <p>
            Venice meets many of the needs of developers new to event-stream
            processing. Venice provides a quick-to-deploy framework that has
            much of the configuration work automated. This means developers can
            get to their tasks of connecting their own data producers and
            implementing queries over their data in minutes. They can also
            verify things are running smoothly using the graphical user
            interface and CLI.
          </p>
          <p>
            However, there are always areas for improvement. The following goals
            would likely add the most value to Venice:
          </p>
          <ul>
            <li>
              Automate deployment and management across distributed servers
            </li>
            <li>Enable auto-scaling (up and down) according to workload</li>
            <li>Add more default pipelines</li>
            <li>Add support for more connectors</li>
            <li>Enable external network storage</li>
            <li>Automate the process of adding producers to the pipeline</li>
          </ul>

          <h2 id="References" class="sec-header">8. References</h2>

            <ol>
              <li id="microservices" class="reference-item">
                S. Newman, <i>Building Microservices</i>, 1st ed. Sebastopol, CA, USA.
                O’Reilly, 2015. <a href="#ref1" class="reference-backref">↩︎</a>
              </li>

              <li id="mssp" class="reference-item">
                M. Kleppman, <i>Making Sense of Stream Processing</i>, 1st ed.
                Sebastopol, CA, USA. O’Reilly, 2016. <a href="#ref2" class="reference-backref">Fig. 2 ↩︎</a>  <a href="#ref3" class="reference-backref">2.1 p.5 ↩︎</a>  <a href="#ref7" class="reference-backref">2.3.2 ↩︎</a>
              </li>

              <li id="two-phase-commit" class="reference-item">
                “Consensus Protocols: Two-Phase-Commit”, The Paper Trail blog,
                <a
                  href="https://www.the-paper-trail.org/post/2008-11-27-consensus-protocols-two-phase-commit/"
                  target="_blank"
                  >https://www.the-paper-trail.org/post/2008-11-27-consensus-protocols-two-phase-commit/</a
                >
                (April 13, 2020). <a href="#ref3" class="reference-backref">↩︎</a>
              </li>

              <li id="ddia" class="reference-item">
                M. Kleppmann, <i>Designing Data Intensive Applications</i>, 1st ed.
                Sebastopol, CA, USA. O’Reilly, 2017. <a href="#ref4" class="reference-backref">2.2 p.1 ↩︎</a> <a href="#ref5" class="reference-backref">2.2 p.3 ↩︎</a> <a href="#ref6" class="reference-backref">2.3.1 ↩︎</a>
                <a href="#ref7" class="reference-backref">2.3.2 ↩︎</a> <a href="#ref9" class="reference-backref">2.3.3 ↩︎</a>
              </li>
  
              <li id="okay-store-data" class="reference-item">
                J. Kreps, “It’s Okay To Store Data In Apache Kafka”, Confluent.io
                blog,
                <a
                  href="https://www.confluent.io/blog/okay-store-data-apache-kafka/"
                  target="_blank"
                  >https://www.confluent.io/blog/okay-store-data-apache-kafka/</a
                >
                (April 16, 2020). <a href="#ref8" class="reference-backref">↩︎</a>
              </li>

              <li id="streaming-systems" class="reference-item">
                T. Akidau, S. Chernyak, and R. Lax, <i>Streaming Systems</i>, 1st ed.
                Sebastopol, CA, USA. O’Reilly, 2018. <a href="#ref10" class="reference-backref">↩︎</a>
              </li>

              <li id="kafka-streams" class="reference-item">
                J. Kreps, “Introducing Kafka Streams: Stream Processing Made
                Simple”, Confluent.io blog,
                <a
                  href="https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/"
                  target="_blank"
                  >https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/</a
                >
                (April 14, 2020). <a href="#ref11" class="reference-backref">↩︎</a>
              </li>

              <li id="streams-vs-ksql" class="reference-item">
                D. Traphagen, “Kafka Streams and ksqlDB Compared - How to Choose”, Confluent.io blog, <a href=”https://www.confluent.io/blog/kafka-streams-vs-ksqldb-compared/” target=”_blank”>https://www.confluent.io/blog/kafka-streams-vs-ksqldb-compared/</a> (March 20, 2020).
                 <a href="#ref12" class="reference-backref">↩︎</a>
              </li>

              <li id="kafka-clients" class="reference-item">
                Apache Kafka documentation: Kafka Clients, <a href=”https://cwiki.apache.org/confluence/display/KAFKA/Clients” target=”_blank”> https://cwiki.apache.org/confluence/display/KAFKA/Clients</a> (April 13, 2020). <a href="#ref13" class="reference-backref">↩︎</a>
              </li>

              <li id="avro" class="reference-item">
                J. Kreps, “Why Avro for Kafka Data?”, Confluent.io blog, <a href=”https://www.confluent.io/blog/avro-kafka-data/” target=”_blank”>https://www.confluent.io/blog/avro-kafka-data/</a> (April 15, 2020). <a href="#ref14" class="reference-backref">↩︎</a>
              </li>

              <li id="docker-compose" class="reference-item">Docker documentation: Docker Compose, <a href=”https://docs.docker.com/compose/” target=”_blank”>https://docs.docker.com/compose/</a> (April 12, 2020). <a href="#ref15" class="reference-backref">↩︎</a></li>
              
              <li id="compose-networking" class="reference-item">Docker documentation: Docker Compose networking, <a href=”https://docs.docker.com/compose/networking/” target=”_blank”>https://docs.docker.com/compose/networking/</a> (April 12, 2020). <a href="#ref15" class="reference-backref">↩︎</a></li>
  
              <li id="compose-startup" class="reference-item">Docker documentation: Docker Compose startup order, <a href=”https://docs.docker.com/compose/startup-order/” target=”_blank”>https://docs.docker.com/compose/startup-order/</a> (April 10, 2020). <a href="#ref16" class="reference-backref">↩︎</a></li>

              <li id="rmoff-tips" class="reference-item">R. Moffatt, “Docker Tips with KSQL and Kafka”, Professional Blog of Robin Moffatt, <a href=”https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-ksql-and-kafka/” target=”_blank”>https://rmoff.net/2018/12/15/docker-tips-and-tricks-with-ksql-and-kafka/</a> (April 9, 2020). <a href="#ref17" class="reference-backref">↩︎</a></li>

            </ol>
        </section>
      </main>
    </div>
    <section class="lower-cta">
      <h2>Ready to get started?</h2>

      <div class="buttons">
        <a href="docs.html" class="button button-link button-wide banner-cta"
          >Docs</a
        >
        <a href="docs.html#Quickstart" class="button button-primary button-wide banner-cta"
          >Quick Start</a
        >
        <a
          href="https://github.com/venice-framework"
          target="_blank"
          class="button button-link banner-cta"
          >GitHub</a
        >
      </div>
    </section>

    <footer>
      <div class="footer-backdrop">
        <section id="our-team">
          <h2>Our Team</h2>
          <p>
            We are looking for opportunities.<br />If you liked what you saw and
            want to talk, please reach out!
          </p>
          <ul>
            <li class="individual">
              <img src="images/team/nancy.png" alt="Nancy Trinh" />
              <h3>Nancy Trinh</h3>
              <p>San Francisco, CA</p>
              <ul class="social-icons">
                <li>
                  <a
                    href="mailto:nancytrinh20@gmail.com?subject=Venice%20Project"
                  >
                    <img src="images/icons/email_icon.png" alt="email" />
                  </a>
                </li>
                <li>
                  <a href="http://github.com/nantrinh" target="_blank">
                    <img src="images/icons/website_icon.png" alt="website" />
                  </a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/nancytrinh/"
                    target="_blank"
                  >
                    <img src="images/icons/linked_in_icon.png" alt="linkedin" />
                  </a>
                </li>
              </ul>
            </li>
            <li class="individual">
              <img src="images/team/David.jpg" alt="David Perich" />
              <h3>David Perich</h3>
              <p>Melbourne, AU</p>
              <ul class="social-icons">
                <li>
                  <a
                    href="mailto:davidnperich@gmail.com?subject=Venice%20Project"
                  >
                    <img src="images/icons/email_icon.png" alt="email" />
                  </a>
                </li>
                <li>
                  <a href="http://www.davidperich.com" target="_blank">
                    <img src="images/icons/website_icon.png" alt="website" />
                  </a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/davidperich/"
                    target="_blank"
                  >
                    <img src="images/icons/linked_in_icon.png" alt="linkedin" />
                  </a>
                </li>
              </ul>
            </li>
            <li class="individual">
              <img src="images/team/Melissa.png" alt="Melissa Manousos" />
              <h3>Melissa Manousos</h3>
              <p>Los Angeles, CA</p>
              <ul class="social-icons">
                <li>
                  <a
                    href="mailto:me@melissamanousos.com?subject=Venice%20Project"
                    target="_blank"
                  >
                    <img src="images/icons/email_icon.png" alt="email" />
                  </a>
                </li>
                <li>
                  <a href="https://melissamanousos.com/" target="_blank">
                    <img src="images/icons/website_icon.png" alt="website" />
                  </a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/melissa-manousos"
                    target="_blank"
                  >
                    <img src="images/icons/linked_in_icon.png" alt="linkedin" />
                  </a>
                </li>
              </ul>
            </li>
          </ul>
        </section>
      </div>
      <div class="design-credit">
        Logo and color scheme:
        <a href="http://linzimurray.creative" target="_blank"
          >linzimurray.creative</a
        >
        • Layout: <a href="http://jkulton.com" target="_blank">Jon Kulton</a>
      </div>
    </footer>
  </body>
</html>
