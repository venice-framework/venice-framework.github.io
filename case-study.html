<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="UTF-8" />
    <title>Venice</title>

    <link rel="stylesheet" href="stylesheets/reset.css" />
    <link rel="stylesheet" href="stylesheets/main.css" />

    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="images/icons/favicons/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="images/icons/favicons/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="images/icons/favicons/favicon-16x16.png"
    />
    <link rel="manifest" href="images/icons/favicons/manifest.json" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      href="https://fonts.googleapis.com/css2?family=Barlow+Condensed:wght@600&family=Fira+Code&family=Open+Sans:ital,wght@0,400;0,600;1,400&display=swap"
      rel="stylesheet"
    />
    <meta property="og:title" content="Venice Case Study" />
    <meta
      property="og:description"
      content="An open-source framework for deploying and managing stream processing pipelines"
    />
    <meta property="og:image" content="/images/thumbnail.png" />
    <meta name="theme-color" content="#0f4c75" />
    <script src="javascripts/sidebar.js"></script>
  </head>

  <body>
    <header class="header-short">
      <nav>
        <ul>
          <li>
            <a href="index.html">
              <img src="images/logos/venice_logo.png" alt="Venice logo" />
            </a>
          </li>
          <li><a href="case-study.html">Case Study</a></li>
          <li><a href="docs.html">Docs</a></li>
          <li><a href="team.html">Team</a></li>
          <li class="flex-float-right">
            <a href="https://github.com/venice-framework" target="_blank">
              <img
                src="images/logos/github-mark-light.png"
                alt="Venice GitHub"
                class="github"
              />
            </a>
          </li>
        </ul>
      </nav>
    </header>

    <div class="study-wrapper">
      <aside class="sidebar">
        <ul>
          <li>
            <a href="#Introduction"> 1. Introduction</a>
          </li>
          <li>
            <a href="#Background"> 2. Background</a>
          </li>
          <li>
            <a href="#Venice"> 3. Venice</a>
          </li>
          <li>
            <a href="#Architecture"> 4. Architecture & Deployment</a>
          </li>
          <li>
            <ul>
              <li>
                <a href="#Docker">
                  4.1 Docker
                </a>
              </li>
              <li>
                <a href="#Kafka-UI">
                  4.2 Kafka UI - Kafdrop
                </a>
              </li>
              <li>
                <a href="#Venice-CLI">
                  4.3 Venice CLI
                </a>
              </li>
            </ul>
          </li>
          <li>
            <a href="#Implementation"> 5. Implementation</a>
          </li>
          <li>
            <ul>
              <li>
                <a href="#Kafka">
                  5.1 Kafka
                </a>
              </li>
              <li>
                <a href="#ksqlDB">
                  5.2 ksqlDB
                </a>
              </li>
              <li>
                <a href="#Kafka-Connect">
                  5.3 Kafka Connect
                </a>
              </li>
              <li>
                <a href="#Schema-Registry">
                  5.4 Schema Registry
                </a>
              </li>
            </ul>
          </li>
          <li>
            <a href="#Challenges"> 6. Challenges</a>
          </li>
          <li>
            <ul>
              <li>
                <a href="#Data-Persistence"> 6.1 Data Persistence</a>
              </li>
              <li>
                <a href="#containers"> 6.2 Containers</a>
              </li>
              <li>
                <a href="#Serialization-Format">
                  6.3 Data Serialization Formats</a
                >
              </li>
            </ul>
          </li>
          <li>
            <a href="#Future-Work"> 7. Conclusion & Future Work</a>
          </li>
          <li>
            <a href="#References"> 8. References</a>
          </li>
        </ul>
      </aside>

      <main>
        <section id="case-study">
          <h1>Case Study</h1>

          <h2 id="Introduction" class="sec-header">1. Introduction</h2>

          <p>
            One of the main challenges for event-driven microservices
            architecture is how the system handles the volume and velocity of
            data that needs to be distributed to different services. Many
            services may rely on the same event data but require it in different
            formats. One solution, event-stream processing, involves processing
            and transforming events as they arrive in the system before
            distributing them to their respective services. Event-stream
            processing decouples data writes from data reads. This means we can
            choose the ideal tools and formats for both reads and writes.
            Event-stream processing is a powerful solution for delivering event
            data in real time by centralizing event transformations in a single
            pipeline.
          </p>

          <p>
            The implementation of these pipelines differs greatly, but
            essentially they have several components that are connected to
            achieve the following workflow:
          </p>

          <ol>
            <li>Event producers generate event messages</li>
            <li>
              The events are transmitted from the producer to the message broker
            </li>
            <li>
              From the broker, they are consumed by the stream-processing engine
              for transformation
            </li>
            <li>Event consumers read the transformed event data</li>
          </ol>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Pipeline-Generalized.png"
              alt="data source feeds message broker feeds stream processing engine feeds data sink"
            />
            <figcaption>
              Figure 1. A high-level overview of a generalized event-stream.
              processing pipeline
            </figcaption>
          </figure>

          <p>
            Developing and deploying an event-stream processing pipeline is
            typically expensive, complex, or both. Venice is an open-source
            framework that enables small teams starting out with event-stream
            processing to quickly deploy and manage an event-stream processing
            pipeline.
          </p>
          <p>
            Venice is built for smaller applications and for developers with
            limited event-streaming knowledge. It uses open source components to
            set up a stream processing pipeline with reasonable default settings
            and simple management tools within minutes.
          </p>
          <p>
            Developers building their first event-stream processing applications
            face some key challenges:
          </p>

          <ol>
            <li>
              Choosing between a number of potential components and creating a
              functioning pipeline.
            </li>
            <li>
              Connecting services to the pipeline to read the processed data.
            </li>
            <li>
              Ensuring consistency and correctness of data at all stages of the
              pipeline.
            </li>
            <li>
              Persisting event data and transformed data to populate
              materialized views and add new sinks as requirements change.
            </li>
          </ol>
          <p>
            Venice abstracts away much of this complexity so that developers can
            focus on application code, rather than building and future-proofing
            this pipeline. Venice is a fully extensible foundation from which
            small streaming applications can grow as requirements shift.
          </p>

          <p>
            This case study outlines the design, architecture, and
            implementation challenges of Venice. The value of Venice may become
            more apparent with insight into some of the problems that events and
            event-stream processing solve for distributed web applications.
          </p>

          <h2 id="Background" class="sec-header">2. Background</h2>

          <p>
            Many applications start out as simple monoliths. Over time, as
            requirements grow and become more complex, the need for scale
            prompts many to shift to a service-oriented architecture, such as
            microservices [<a href="#microservices">1, Ch. 1</a>].
          </p>

          <p>
            One of the goals of microservices is to decouple services from each
            other. However, multiple services often require access to the same
            underlying data, including historical data.
          </p>

          <p>
            How might an organization
            <b>propagate data to multiple services in real-time</b> without
            sacrificing <b>availability</b><sup>1</sup> or <b>consistency</b>,
            while retaining the flexibility to <b>use historical data</b>?
          </p>

          <p class="note">
            <sup>1</sup>Availability here means the opposite of downtime. If a
            system is responsive it is available.
          </p>

          <h3>
            2.1 It is difficult to share data between distributed services
          </h3>

          <p>
            An application might resemble this: a jumble of requests and data
            propagations between the application, a primary database, data
            warehouse, business analytics engine, search index, cache, and graph
            database.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/PrimaryDatabaseDependency.png"
              style="max-height: 400px;"
              alt="A web application sends data to a primary database that feeds multiple other databases and processes"
            />
            <figcaption>
              Figure 2. The same data is needed for many different purposes.
              <sup>1</sup>
            </figcaption>
          </figure>

          <p class="note">
            <sup>1</sup> Figure inspired by [<a href="#mssp">2, Ch 2</a>].
          </p>

          <p>
            There are numerous ways to use data from this application, and
            different tools are better suited for different situations. For
            example, an e-commerce application might use orders data to
            continuously update:
          </p>
          <ul>
            <li>
              A cache of “deals of the day” and remaining inventory, to avoid
              overloading the database with redundant queries.
            </li>
            <li>
              A search index, to enable full-text search of available products.
            </li>
            <li>
              A key-value store of personalised product recommendations for
              customers.
            </li>
          </ul>

          <p>
            These outputs are all <b>derived views</b> of the same underlying
            data. Typically, it is difficult to maintain real-time services –
            such as the stock inventory, search index, or product recommendation
            services from above –
            <b
              >without sacrificing the availability of the services or
              consistency of data</b
            >. When a product runs out of stock, this change would ideally be
            propagated instantly to all these services, instead of hourly or
            daily. From a user and business perspective, it is desirable to
            update the shop as close to real time as possible. This would
            prevent displaying out-of-stock products to users.
          </p>

          <p>
            Dual writes are one way to update data across multiple services.
            However, partial failures, such as a network outage, can result in
            permanent inconsistency. Imagine writing orders to one database and
            updating remaining inventory in a different database. If a new order
            comes in for Product X, and is written successfully to the orders
            database, but the network crashes before the remaining inventory is
            updated, the two data stores are now inconsistent, and will remain
            that way without a manual recount.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/DualWriteFailure.png"
              style="max-height: 400px;"
              alt="A customer order is written to one database but the accompanying inventory update fails"
            />
            <figcaption>
              A successful write to one database and a failed write to the
              second.
            </figcaption>
          </figure>

          <p>
            Distributed transactions can provide consistency, but may sacrifice
            availability, because partial failures can render the application
            extremely slow or unresponsive [<a href="mssp">2, Ch. 2</a>;
            <a href="two-phase-commit">3</a>].
          </p>

          <p>
            To address this challenge, some companies have turned to using
            events to propagate data changes between their services.
          </p>

          <h3>2.2 Events drive application behavior</h3>

          <p>
            An event is an immutable object that describes something that
            happened at some point in time in an application [<a href="#ddia"
              >4, Ch. 11</a
            >]. For example, if a new order is placed, that order can be modeled
            as an event as follows:
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/OrderEventObject.png"
              style="max-height: 400px;"
              alt="order event object with properties for order_id, customer_id, seller_id, product_id, quantity, price_in_cents, timestamp"
            />
            <figcaption>Figure 3. An example of an order event.</figcaption>
          </figure>

          <p>
            An event can trigger one or more actions. For example, when a new
            order comes in, the event can trigger an update to the inventory,
            start the fulfillment process, and send an email notification to the
            customer.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/OrderTriggersServices.png"
              style="max-height: 400px;"
              alt="data from the order object in the previous figure is sent to three different services"
            />
            <figcaption>
              Figure 4. A new order triggers the inventory management,
              fulfillment, and notifications services.
            </figcaption>
          </figure>

          <p>
            In streaming terminology, a producer generates an event. Related
            events are grouped together into a stream (also known as a topic).
            That event is then processed by one or many consumers [<a
              href="#ddia"
              >4, Ch. 11</a
            >]. In the example above, the application is the producer. When a
            customer creates a new order, the application produces an event to
            the orders stream. The inventory management, fulfillment and
            notifications services are the consumers who receive the event and
            take appropriate action.
          </p>

          <h3>2.3 Message brokers move data from producers to consumers</h3>

          <p>Message brokers are suited for situations where:</p>

          <ul>
            <li>producers and consumers are asynchronous.</li>
            <li>
              multiple producers may write to a topic, and multiple consumers
              may read from a topic.
            </li>
          </ul>

          <p>
            There are two common types of message brokers: (1) message queues
            (brokers that implement the AMQP and JMS standards) and (2)
            log-based message brokers.
          </p>

          <h4>
            2.3.1 Message queues do not retain events and may process events out
            of order
          </h4>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/gifs/MessageQueue.gif"
              style="max-height: 400px;"
              alt="gif of a message queue with messages enqueing and dequeuing"
            />
            <figcaption>
              Figure 5. A message queue: new messages enqueue and old messages
              dequeue.
            </figcaption>
          </figure>

          <p>
            Message queues (e.g., RabbitMQ, ActiveMQ) are preferable in
            situations where:
          </p>

          <ul>
            <li>each event may take a long time to process.</li>
            <li>processing order is not important.</li>
            <li>event retention is not required.</li>
          </ul>

          <p>
            Recall that the motivating question was: How might an organization
            propagate data to multiple services in real-time without sacrificing
            availability or consistency, while retaining the flexibility to use
            historical data?
          </p>

          <p>
            Message queues are not the answer because they may process events
            out of order and do not retain events [<a href="#ddia">4, Ch. 11</a
            >].
          </p>

          <h4>
            2.3.2 Log-based message brokers retain events and help guarantee
            order
          </h4>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Log.png"
              style="max-height: 400px;"
              alt="a row of numbers in boxes, each box represents a message written to the log"
            />
            <figcaption>
              Figure 6. A log-based message broker: new messages are appended to
              an immutable log.
            </figcaption>
          </figure>

          <p>
            Log-based message brokers (e.g., Apache Kafka, Amazon Kinesis
            Streams) are preferable in situations where:
          </p>

          <ul>
            <li>each event can be processed quickly.</li>
            <li>processing order is important.</li>
            <li>
              event retention is required [<a href="#mssp">2, Ch. 2</a>;
              <a href="#ddia">4, Ch. 11</a>].
            </li>
          </ul>

          <p>
            Log-based message brokers are the answer to the motivating question.
          </p>

          <ol>
            <li>
              <b>Data can be propagated to services in real-time.</b><br />
              Logs support low latency and high throughput writes and reads.<br />
              <ul>
                <li>
                  Low Latency: Each write is an append operation, and a read is
                  a linear scan. Producers can write data to the log in a single
                  step, and consumers can read data from the log very quickly.
                </li>
                <li>
                  High Throughput: Logs can be split into partitions and
                  replicated across machines. This means many reads and writes
                  can proceed concurrently [<a href="#okay-store-data">5</a>].
                </li>
              </ul>
            </li>
            <li>
              <b>The system is fault-tolerant.</b>
              <ul>
                <li>
                  With multiple brokers, messages are rerouted to replicas if
                  one crashes.
                </li>
                <li>
                  With partitioned data and load-balanced consumers, if one
                  consumer fails, others can pick up the work.
                </li>
              </ul>
            </li>
            <li>
              <b>Data is consistent across services.</b>
              <ul>
                <li>
                  Events are ordered within partitions. Keyed partitions route
                  events with the same key to the same partition, ensuring
                  order. For example, partitioning by product ID allows all
                  orders for Product X to be processed in the order they are
                  produced.
                </li>
                <li>
                  Logs are immutable. Services that require the same underlying
                  data will all see the same data. This avoids inconsistencies
                  due to race conditions associated with having multiple
                  databases.
                </li>
              </ul>
            </li>
            <li>
              <b>Historical data is preserved.</b>
              <ul>
                <li>
                  Logs contain historical information and new information. New
                  services can be added to the system at any time, consume the
                  historical data, and follow future events automatically.
                </li>
                <li>
                  Developing new services is also possible, with the option to
                  gradually transition, or roll back changes.
                </li>
              </ul>
            </li>
          </ol>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/LogAsBroker.png"
              style="max-height: 400px;"
              alt="generic data sources send data to a log-based message broker that feeds into a data sink"
            />
            <figcaption>
              Figure 7. Event producers send messages to a partitioned log-based
              broker. Event consumers use these event messages in various ways.
            </figcaption>
          </figure>

          <h4>2.3.3 A note on the limitations of logs</h4>
          <p>
            Log-based approaches are not the answer to all problems. Events are
            consumed in a linear fashion, so a single event taking a long time
            to process could delay processing of later events. [<a href="#ddia"
              >4, Ch. 11</a
            >]. Ordering of events is only guaranteed within an individual
            partition, which may be problematic as the system scales. The system
            also does not support reading self writes [<a href="#ddia"
              >4, Ch. 5</a
            >], and other linearizability guarantees [<a href="#ddia"
              >4, Ch.9, Ch. 12</a
            >]. In many cases, the benefits outweigh the drawbacks, and
            log-based message brokers present a powerful solution to the problem
            of <b>propagating data to multiple services in real-time</b>,
            without sacrificing <b>availability</b> and <b>consistency</b>,
            while retaining the flexibility to <b>use historical data</b>.
          </p>

          <h3>
            2.4 Stream processors abstract common operations out to a fast and
            fault-tolerant engine
          </h3>

          <p>
            It is common to have multiple services use the same data to generate
            different outputs in an application. The outputs generated may be
            different, but many of the operations used by the services to
            process these events are the same. These operations include
            windowing, aggregation, joins, filters, and transformations [<a
              href="#streaming-systems"
              >6</a
            >].
          </p>

          <p>
            Individual consumers can be written to perform these operations.
            This works well for operations such as filtering or transforming
            that process one message at a time. This approach becomes more
            challenging when operations are more complex, for example,
            aggregating over time or joining two different streams together.
          </p>

          <p>
            Event stream processors abstract these common processing operations
            to an engine. Such an engine can provide additional benefits such as
            fault tolerance, efficient processing of events via clusters,
            maintenance of local state, and out-of-order events handling [<a
              href="kafka-streams"
              >7</a
            >].
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/gifs/ProcessorReplacesConsumers.gif"
              style="max-height: 400px;"
              alt="gif of several consumers being replaced by a single stream processing engine"
            />
            <figcaption>
              Figure 8. An event-stream processing engine replaces the need for
              individual consumers performing redundant processing tasks.
            </figcaption>
          </figure>

          <p>
            Adding a stream processor to a log-based architecture expands the
            benefits of the system. Services can write data efficiently by
            appending it to the log. The stream processor can complete
            operations shared by different services more efficiently. This means
            that services can share the same underlying data and easily
            transform that data into a format that is ideal for their needs.
          </p>

          <h4>2.4.1 A note on the importance of databases</h4>

          <p>
            This background emphasizes the benefits of an event-driven
            architecture with a distributed log at its core. However, event
            driven architecture does not replace databases completely.
          </p>

          <p>
            Databases prioritize state, while events prioritize state changes.
            State is derived from events – for example, your physical location
            as you read this is the result of the event that moved you there. In
            an application, both events and state are useful. Databases in an
            architecture provide fast access to state.
          </p>

          <p>
            A log-based, event-driven architecture utilizes distributed logs and
            databases to provide the benefits of both in a fault-tolerant and
            scalable way.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Brokers-Processor.png"
              style="max-height: 400px;"
              alt="producers or sources such as user activity, sensor data, or database writes feed events to a replicated log-based message broker which sends them to a stream processing engine and eventually they wind up in various consumers or sinks such as a cache, graph database, or search index"
            />
            <figcaption>
              Figure 9. An example of a log-based stream processing
              architecture.
            </figcaption>
          </figure>

          <h3>2.5 Summary</h3>

          <ul>
            <li>
              In a microservices architecture, many services
              <b>require access to the same data</b>.
            </li>
            <li>
              Propagating data to many services is difficult with architecture
              centered on <b>databases</b>.
            </li>
            <li>
              <b>Events</b> can help address this problem. Events represent what
              happened in the application and when.
            </li>
            <li>
              <b>Message brokers</b> help move events between
              <b>producers</b> (data creators, such as an application) and
              <b>consumers</b> (services that need the data).
            </li>
            <li>
              <b>Log-based message brokers</b> can propagate data to services in
              real time in a fault-tolerant, consistent, and scalable way, and
              provide the flexibility to use historical data.
            </li>
            <li>
              <b>Event stream processors</b> abstract out common operations on
              event data to a fault-tolerant and scalable engine that maintains
              event order.
            </li>
            <li>
              An architecture with a distributed event log at its core and
              databases to store outputs provides
              <b>fast access to both events and state</b>. Multiple services can
              now use the same data in different ways, and the application has
              the agility to grow with changing requirements.
            </li>
          </ul>

          <h2 id="Venice" class="sec-header">
            3. Venice
          </h2>

          <p>
            Venice is an open-source framework that enables developers starting
            out with event-stream processing to quickly deploy and manage an
            event-stream processing pipeline.
          </p>

          <p>
            Venice is built for smaller applications and for developers with
            limited event-streaming knowledge. It uses open source components to
            set up a stream processing pipeline with reasonable default settings
            and simple management tools within minutes.
          </p>

          <p>
            Large companies typically use stream processing to perform the
            following tasks:
          </p>

          <ul>
            <li>
              Process complex real-time event data to make applications more
              responsive.
            </li>
            <li>Detect anomalies in events.</li>
            <li>
              Deliver real-time analytics without interfering with the
              application’s ability to write new data.
            </li>
          </ul>

          <p>
            The desire to perform these tasks, however, is not unique to large
            enterprises with applications that process millions of events per
            day. Any growing application might benefit from event stream
            processing. The implementation of these pipelines differs, but they
            have several interconnected components that achieve the following
            workflow:
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/Pipeline-Generalized.png"
              alt="one or more event processors (data sources) feed into a message broker (typically Kafka - a log) which feeds into a stream processing engine ending at event consumers also known as data sinks (visualization tools, caches, databases, Kafka topic, etc.)"
            />
            <figcaption>
              Figure 10. A generalized event-stream processing pipeline.
            </figcaption>
          </figure>

          <h3>3.1 Venice sits between DIY and managed solutions</h3>

          <p>
            The existing choices available to developers roughly fall into two
            categories: managed solutions and do-it-yourself (DIY).
          </p>

          <p>
            Managed solutions (such as Confluent’s Cloud Platform or Landoop)
            empower developers to focus on their applications by offering
            configuration guidance and on-going support for teams implementing
            streaming systems. However, they have high price tags, are generally
            designed for large enterprises, and can lock the team into using one
            vendor.
          </p>

          <p>
            The DIY approach is an option for teams newer to this space.
            However, it trades monetary cost for complexity. First, application
            developers must deal with an abundance of choice. For the stream
            processing engine alone, there are multiple open-source options and
            several paid ones (figure 11).
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/streamProcessingEngines.png"
              alt="logos for AWS Kinesis, Google CloudData, and Apache products: Samza, Spark, Flink, Apex, Flume, Heron, Storm, Kafka Streams, Beam, ksqlDB"
            />
            <figcaption>
              Figure 11. A sampling of the options for stream processing
              engines.
            </figcaption>
          </figure>

          <p>
            Second, because the stream-processing space is emerging,
            configuration tutorials become quickly outdated and documentation is
            minimal or challenging to parse. Choosing and learning how to
            configure a single component in the pipeline could take days or
            weeks. The problem compounds when attempting to learn which
            components best integrate together to form a pipeline with tradeoffs
            that are acceptable for the project.
          </p>

          <p>
            Existing managed solutions and open source options may not be
            suitable for cost-conscious developers with limited streaming
            knowledge developing small streaming applications. Venice aims to be
            the middle ground - balancing the need for affordability, simplicity
            and control.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/VeniceVsDIY-Managed.png"
              alt="Three arrows representing cost, ease of use, and user control with Venice, DIY, and managaged solutions arranged along them accordingly"
            />
            <figcaption>
              Figure 12. Venice positions itself between DIY and managed
              solutions in terms of cost, ease of use, and user control.
            </figcaption>
          </figure>

          <!-- style this section & add icons for clock and plus -->
          <h3>
            3.2 Design goals for an approachable stream processing framework
          </h3>

          <div class="design-goals">
            <p>
              Venice is approachable for developers new to event-stream
              processing. It allows them to:
            </p>
            <ul>
              <li>
                <!-- clock icon -->Set up common pipeline patterns in minutes so
                they can minimize time spent on configuration and focus on
                application code instead.
              </li>
              <li>
                <!-- plus icon -->Retain extensibility and potential for
                reconfiguration if their needs change or become more complex.
              </li>
            </ul>
          </div>

          <h2 id="" class="sec-header"></h2>

          <h2 id="" class="sec-header"></h2>

          <h2 id="Challenges" class="sec-header">6. Challenges</h2>

          <p>
            Venice solves for several challenges: persisting data, reliably
            launching and connecting component containers, and consistently
            serializing data.
          </p>

          <h3 id="Data-Persistence">
            6.1 How can Venice persist data to make materialized views and new
            services possible?
          </h3>
          <p>
            This challenge has three parts: data persistence, storage choice,
            and log retention strategy.
          </p>

          <h4>6.1.1 How should Venice persist data?</h4>

          <p>
            Retaining events indefinitely in Kafka allows users to build new
            services or re-design existing services that can consume the entire
            history of events to recreate application state. This is a critical
            feature for teams new to streaming applications who will be
            experimenting to discover what works.
          </p>

          Docker provides two ways to persist data for containers -
          <b>volumes</b> and <b>bind mounts</b>.

          <figure class="img-wrapper">
            <img
              src="images/diagrams/DockerVolumes_bindMounts.png"
              alt="An arrow points from the Docker container to the Docker area within the host machine created by a volume. A second arrow points to host machine in general to represent a bind mount."
            />
            <figcaption>
              Figure XX. Docker containers interact with the host machine’s file
              system through bind mounts or volumes.
            </figcaption>
          </figure>

          <p>
            The main difference, shown in figure xxxx, is where data is saved on
            the host machine. With volumes, Docker manages storing data in a
            restricted Docker directory. With a bind mount, a specific path on
            the host machine is loaded on the container at startup.
          </p>

          <p>
            Venice uses bind mounts to ensure Docker loads a specific path from
            the host machine into the container everytime a container starts.
            When using volumes, new volumes are created when containers are
            re-configured and rebuilt. Bind mounts are a more reliable and
            consistent way of reloading data into the container for developers
            experimenting with new pipelines.
          </p>

          <h4>
            6.1.2 What is the optimal way for Venice to provide event
            persistence?
          </h4>

          <p>
            There are three options for persisting Kafka events indefinitely:
            permanent log retention, external backups, and compacted topics.
          </p>

          <h5>Choice 1: Permanent Log Retention</h5>

          <p>
            Typically, Kafka will delete events after a predetermined amount of
            time (e.g., 7 days) or when the log reaches a certain byte size.
            However, permanently preserving events is possible by overriding the
            default retention settings. A new consumer could then consume
            everything from the beginning of the log to derive the current
            state.
          </p>

          <h5>Choice 2: External Backup</h5>

          <p>
            Another option is to periodically copy older events to an external
            data store and remove them from Kafka. To recreate state for a new
            consumer, the backed up data must be reloaded and configured in
            Kafka.
          </p>

          <h5>Choice 3: Compacted Topics</h5>

          <p>
            Finally, log compaction refers to retaining the last known value for
            each message <b>key</b> within a single topic partition. Compaction
            guarantees event order by only removing older events whose state has
            changed.
          </p>

          <figure class="img-wrapper">
            <img
              src="images/diagrams/TopicCompaction.png"
              alt="Two event objects. One has the email adddress 'daviddd@gmail.com'; the other, 'david@gmail.com'. A box around the second indicates it will be preserved."
            />
            <figcaption>
              Figure XX. An example of a user updating their email address. With
              a compacted topic only the latest value related to the
              <code>user_id: 1</code> is retained.
            </figcaption>
          </figure>

          <p>
            For example, figure xxx shows events that track a change to a user’s
            email, where the user_id is the key. With a compacted topic, Kafka
            would guarantee that the most recent update to the email would be
            kept in the log.
          </p>

          <p>
            Log compaction can be suitable for some topics. However, it means
            losing the ability to <b>recreate</b> state, leaving only the option
            to <b>restore</b> the latest state.
          </p>

          <h5>Venice: Permanent Log Retention</h5>

          <p>
            Venice uses permanent log retention because (1) it is a simpler
            solution than external backup, and (2) log compaction does not not
            fulfill the goal of extensibility. A compacted topic means users
            would not be able to consume historical data.
          </p>

          <p>
            Permanent log retention is a safe option: users have all of their
            events stored by default, but they can choose to turn on log
            compaction for individual topics if they decide that is appropriate
            for their use case.
          </p>

          <p>
            However, as a user’s application grows and events accumulate, they
            may want to consider external backup to free up space on the
            machines running the brokers.
          </p>

          <ol>
            <li>
              The Docker Engine, which is a server that typically runs as a
              daemon on a so-called “Docker host”
            </li>
            <li>
              The Docker REST API, which clients can use to issue Docker
              commands to a Docker host
            </li>
            <li>
              The Docker CLI, which uses the REST API to interact with whatever
              Docker host it is connected to
            </li>
          </ol>

          <p>
            From the perspective of most users, Docker just is the CLI. When you
            install Docker locally, a local Docker daemon is started and the CLI
            transparently connects to it via a Unix domain socket. It’s easy to
            miss the fact you are using a client that issues API requests to a
            server. But that’s what is happening under the hood, and it means
            that a Docker client can just as easily connect to a
            <em>remote</em> Docker host.
          </p>

          <p>
            Because the Mothership application – which is in charge of deploying
            and managing applications – runs on a different server from the one
            in which application containers run, it needs the ability to issue
            commands to a remote Docker host. Docker’s client-server
            architecture makes this use-case possible. Mothership’s Mission
            Control server can connect remotely to the Docker host on a separate
            server to build and run containers, as illustrated in the following
            diagram.
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/remote-docker.png"
              alt="Mission Control service connecting to remote Docker host"
            />
          </div>

          <p>
            Here we see the Mission Control service (Mothership’s core Node.js
            application) using a custom Docker client to manage multiple app
            containers on a remote Docker host. Mission Control acts as a proxy
            connecting users of the PaaS to the server responsible for hosting
            apps.
          </p>

          <p>
            Having chosen to use Docker for app isolation, the remaining
            question is how we determine which system and application
            dependencies to install in each application’s container, what
            command we run to start an application, and how we instruct Docker
            to do all this.
          </p>

          <h3 id="Dockerfiles" class="sec-header">5.3 Dockerfiles</h3>

          <p>
            Early versions of Mothership supported only Rack-compliant
            applications written in the latest version of Ruby. Generating an
            appropriate container for such applications was fairly
            straightforward:
          </p>

          <ol>
            <li>Install the latest version of Ruby inside the container</li>
            <li>Copy the app source code into the container</li>
            <li>
              Run <code>bundle install</code> inside the container to install
              app dependencies
            </li>
            <li>
              Start the Puma app server (a Rack compliant app server that can
              also serve static assets)
            </li>
          </ol>

          <p>
            We used a custom Dockerfile to give these instructions to Docker: a
            Dockerfile is simply an imperative set of instructions for
            generating a Docker container. It was easy enough to make Mothership
            more flexible by checking the source code for a specific Ruby
            version and dynamically adjusting the Dockerfile. But a key goal of
            Mothership was to handle multiple languages and frameworks.
          </p>

          <p>
            One way of handling this is to create a custom Dockerfile for each
            language. However, this approach quickly gets unmanageable: it is
            brittle, requires a deep understanding of the needs of applications
            written in different languages, and requires constant maintenance.
            While possible, this approach was not feasible for our small team. A
            better solution comes from Heroku: the <em>buildpack</em>.
          </p>

          <h3 id="Buildpacks" class="sec-header">5.4 Buildpacks</h3>

          <p>
            Like a Dockerfile, a buildpack contains a set of instructions for
            generating a suitable environment in which an application can run.
            The benefit of buildpacks, however, is that there are well-tested,
            open-source buildpacks that support applications written in many of
            the most popular languages and frameworks. Additionally, there are
            standard ways of examining source code and determining which
            buildpack is appropriate. Although they originated for use within
            Heroku’s proprietary system, at this point they are effectively an
            industry standard.
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/buildpacks.png"
              style="max-height: 150px;"
              alt="Buildpacks executed against application source code, resulting in prepared environment"
            />
          </div>

          <p>
            Traditionally, one or more buildpacks are installed on a host
            operating system, then the buildpacks are executed against a
            directory of application source code. The result of this
            <em>build process</em> is a ready to run environment. Different
            buildpack systems result in different outputs, but all result in an
            environment ready to run the application. How then can buildpacks be
            used in conjunction with Docker containers?
          </p>

          <h3 id="Buildpacks-and-Containers" class="sec-header">
            5.5 Buildpacks and Containers
          </h3>

          <p>
            When an application is deployed Mothership starts with a base Docker
            image that has all supported buildpacks installed. Next, it copies
            the source code for the user’s application into the Docker
            container. Then, it executes the buildpacks against the application
            source code. The result of this process is stored as a finalized and
            ready-to-run Docker image.
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/buildpacks-to-image.png"
              style="max-height: 200px;"
              alt="Copy code to server and execute buildpacks against them"
            />
          </div>

          <p>
            Using buildpacks to generate Docker images allowed us to focus on
            the architecture and features of Mothership, rather than worrying
            about the details of containerizing each different application type.
            However, buildpacks are used internally by Heroku to create
            <em>dynos</em>, not Docker containers. So Mothership requires a
            system for generating Docker containers from buildpacks.
          </p>

          <p>
            To achieve this, we considered an interesting new open-source
            initiative from Heroku and Pivotal (and recently accepted into the
            Cloud Native Foundation’s Sandbox): Cloud Native Buildpacks. Cloud
            Native Buildpacks is essentially a specification for how to convert
            buildpacks into
            <a
              href="https://www.opencontainers.org/about"
              target="_blank"
              rel="noopener"
              >OCI images</a
            >
            in a standard way. Unfortunately, at the moment the implementations
            of this standard are experimental, and the standard itself is
            undergoing rapid development. While Cloud Native Buildpacks are an
            exciting new development, we decided that the technology is still
            too immature to build Mothership on top of.
          </p>

          <p>
            Fortunately, the long-standing open-source project Herokuish offers
            a reliable way of generating Docker images from buildpacks. In our
            testing, Herokuish was very reliable for the types of applications
            Mothership is designed for, and so we built it into Mothership.
          </p>

          <p>
            With a solid containerization strategy in place, the next challenges
            to consider concern resource scheduling.
          </p>

          <h2 id="Resource-Scheduling" class="sec-header">
            6. Resource Scheduling
          </h2>

          <p>Recall that our current architecture looks like the following:</p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/remote-docker.png"
              alt="Detailed comparison showing abstractions"
            />
          </div>

          <p>
            Clearly, this doesn’t scale very well. Depending on the initial size
            of application node, we will quickly run out of resources for
            deploying applications (in our testing, entry-level IaaS nodes run
            low on memory once four database-backed applications are deployed).
            To grow beyond a handful of applications, the PaaS will need to
            support either <em>vertical</em> or <em>horizontal</em> scaling.
          </p>

          <p>
            To implement vertical scaling, we could add functionality for users
            to increase the size (RAM, storage, and CPU) of the application
            node. Unfortunately, vertical scaling can get very costly very
            quickly, and there are still hard limits on how large you can scale
            with this approach.
          </p>

          <h3 id="Horizontal-Scaling">6.1 Horizontal Scaling</h3>

          <p>
            For this reason, Mothership is designed to scale horizontally.
            Instead of a single application node, Mothership supports an
            arbitrary-sized <em>fleet</em> of application nodes. As nodes fill
            up, whoever is in charge of managing the PaaS can simply request
            that Mothership add another node to the fleet.
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/horizontal-scaling-naive.png"
              style="max-height: 450px;"
              alt="Multiple application servers"
            />
          </div>

          <p>
            A fleet of application nodes raises some new questions that our PaaS
            must answer:
          </p>

          <ul>
            <li>Which node should we place newly deployed applications on?</li>
            <li>Which node is a particular application running on?</li>
            <li>
              If a node is added to the fleet, should we rebalance application
              distribution?
            </li>
            <li>
              If a node crashes, how do we redistribute its applications on to
              other nodes?
            </li>
          </ul>

          <h3 id="Container-Orchestration">6.2 Container Orchestration</h3>

          <p>
            These are (some of the) questions of
            <a
              href="https://blog.newrelic.com/engineering/container-orchestration-explained/"
              target="_blank"
              rel="noopener"
              ><em>container orchestration</em></a
            >. The standard way of addressing them is by adding a
            <em>container orchestrator</em> to the system.
          </p>

          <p>
            To understand container orchestrators, we need to introduce the
            concept of a <em>service</em>. When, for example, we deploy a simple
            Node application to Mothership, it is ultimately run as a container.
            But the particular running container is not to be
            <em>identified</em> with the application – it is merely an ephemeral
            <em>instance</em> of the application. If a container crashes, we can
            start a new container and the application lives on. The application
            is a <em>service</em>.
          </p>

          <p>
            The service is one of the key abstractions provided by a container
            orchestrator. We tell the container orchestrator which services
            should be on our system, and the container orchestrator is
            responsible for starting appropriate containers and distributing
            them amongst the nodes. If a container dies, the orchestrator must
            discover this (typically via heartbeat checks) and start a new
            container. It is also responsible for keeping track of available
            nodes and how to network them.<sup class="footnote-ref"
              ><a href="#fn1" id="fnref1">[1]</a></sup
            >
            With a container orchestrator in place, our architecture is as
            follows:
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/multi-tenant-with-orchestrator.png"
              style="max-height: 500px;"
              alt="Detailed comparison showing abstractions"
            />
          </div>

          <p>
            Here, we see that Mission Control talks only to the container
            orchestrator. The container orchestrator is responsible for
            distributing app containers across the servers that house apps. We
            call these servers <em>app pods</em>, and, collectively, they form
            Mothership’s <em>app convoy</em>. Mothership’s container
            orchestrator manages the convoy.
          </p>

          <h3 id="Docker-Swarm">6.3 Docker Swarm</h3>

          <p>
            The two dominant container orchestrators are Docker Swarm and
            Kubernetes.<sup class="footnote-ref"
              ><a href="#fn2" id="fnref2">[2]</a></sup
            >
            We elected to use Docker Swarm for Mothership’s container
            orchestration for several reasons:
          </p>

          <ul>
            <li>It is built into Docker</li>
            <li>
              It has a relatively low learning curve for anyone familiar with
              Docker
            </li>
            <li>We can interact with it directly via the Docker REST API</li>
            <li>
              It handles 95% of our use-case out of the box, and a little elbow
              grease on our part took care of the remaining 5% (see Service
              Discovery, below)
            </li>
          </ul>

          <p>
            The three key concepts in Docker Swarm are <em>nodes</em>,
            <em>services</em>, and <em>tasks</em>. For our purposes, a node is a
            server that is connected to the swarm and has Docker installed.
            There are two types of nodes: <em>worker</em> nodes and
            <em>manager</em> nodes. Worker nodes are only responsible for
            housing running containers, while manager nodes are responsible for
            the actual container orchestration work described above.<sup
              class="footnote-ref"
              ><a href="#fn3" id="fnref3">[3]</a></sup
            >
          </p>
          <p>
            We introduced the concept of a service earlier, and noted that they
            are instantiated by containers. In Docker Swarm, the containers that
            instantiate a service are called tasks.<sup class="footnote-ref"
              ><a href="#fn4" id="fnref4">[4]</a></sup
            >
            To deploy a service to the swarm, we describe the service to the
            manager node and how many tasks the service should have. The manager
            node then assigns the tasks to the worker nodes. Worker nodes keep
            track of the state of the tasks assigned to them – if a container
            dies, the worker must notify the manager node so that it can decide
            what to do.
          </p>

          <p>
            Let’s make this discussion less abstract with an example. When a
            user deploys, e.g. a Node app onto Mothership, Mothership uses a
            buildpack to generate an appropriate Docker image for the app. Then,
            using remote API calls, it tells the manager node from the
            application fleet that there should be a new service, and which
            Docker image should be used to start tasks for that service. The
            manager node then inspects the state of its nodes and assigns the
            service task to an available node. The deployed Node app is now
            running on that node.
          </p>

          <p>
            Suppose something goes wrong and the Node app crashes and shuts
            down. This means the container also shuts down. Suddenly, our
            application is no longer running. The worker node will quickly
            discover that the task it was assigned is no longer running, and it
            will inform the manager node. The manager node knows that we
            requested one task be kept available for this service, and so it
            will once again assign that task to an available node. Within
            seconds, our Node app is back up and running.
          </p>

          <h3 id="App-Services-and-Database-Services">
            6.4 App Services and Database Services
          </h3>

          <p>
            Mothership supports database-backed applications. While it is
            technically possible to have the application and database running
            inside of a single container, this violates the Docker model of
            <a
              href="https://runnable.com/docker/rails/run-multiple-processes-in-a-container"
              target="_blank"
              rel="noopener"
              >single-process containers</a
            >. Instead, when a user requests a database for an application, we
            create a separate Postgres image and instruct the manager node to
            create a new database service. How application services are able to
            communicate with their associated database service is covered below
            in the Service Discovery section. This keeps applications and
            databases decoupled, and allows for easy application scaling.<sup
              class="footnote-ref"
              ><a href="#fn5" id="fnref5">[5]</a></sup
            >
          </p>

          <h3 id="Application-scaling">6.5 Application scaling</h3>

          <p>
            When we describe a service to the manager, we can stipulate how many
            tasks we want for the service. The result is as many containers
            running as we requested. This works best for stateless services.
            Fortunately, web applications are inherently stateless, so
            horizontally scaling apps on Mothership is as simple as issuing a
            single API request to the manager node. Users of Mothership can
            scale out their apps by simply requesting an increased number of
            instances.
          </p>

          <h2 id="Service-Discovery" class="sec-header">
            7. Service Discovery
          </h2>

          <p>
            Now that we have a container orchestrator dynamically distributing
            tasks across our fleet of application nodes, we have two additional
            problems to solve:
          </p>

          <ul>
            <li>
              How do services talk to one another? In particular, how does an
              app talk to its database?
            </li>
            <li>
              How do we route incoming requests from the Internet to the
              appropriate service?
            </li>
          </ul>

          <p>These are problems of <em>service discovery:</em></p>

          <blockquote>
            <p>
              [A] modern microservice-based application typically runs in a
              virtualized or containerized environment where the number of
              instances of a service and their locations changes dynamically.
              Consequently, you must implement a mechanism that enables the
              clients of service to make requests to a dynamically changing set
              of ephemeral service instances.
              <a
                href="https://microservices.io/patterns/server-side-discovery.html"
                target="_blank"
                rel="noopener"
                >link</a
              >
            </p>
          </blockquote>

          <h3 id="Inter-Service-Communication">
            7.1 Inter-Service Communication
          </h3>

          <p>
            Application services need to be able to communicate with their
            associated database services. The trouble is, the tasks for an
            application’s database are dynamically allocated across a cluster of
            nodes. In Docker Swarm, there is no guarantee that the tasks for an
            application and its database are running on the same node. What’s
            more, both the IP address of the database container itself, as well
            as the IP address of its host node, can change at any time. The
            following diagram illustrates the problem:
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/inter_service_communication.png"
              style="max-height: 600px;"
              alt="Diagram showing app and database living on separate nodes"
            />
          </div>

          <p>
            Here, we see that app3 needs to communicate with its database, but
            these services are not currently connected and in fact are on
            different servers.
          </p>

          <p>
            Recall that Docker containers are isolated environments that view
            themselves as complete operating systems. This means that the only
            way for two Docker containers to communicate is via a network
            connection. When we’re talking about multiple containers running on
            the same Docker host, this can be accomplished by creating a virtual
            LAN using a Linux bridge
            <a
              href="https://success.docker.com/article/networking#thelinuxbridge"
              target="_blank"
              rel="noopener"
              >link</a
            >. A Linux bridge is a virtual implementation of a physical network
            switch. By default, Docker creates a so-called “bridge” network
            built on top of Linux bridge technology. All containers created on
            the host are attached to this network by default, and so are able to
            communicate with one another. This, in fact, is how Docker Compose
            connects services in local development.
          </p>

          <p>
            The advantage of this networking model is security. By communicating
            over a virtual LAN instead of exposing ports on the Internet,
            containers remain isolated from the outside world. We want this same
            level of security for our application and database containers. A
            Linux bridge, however, cannot help us here for the simple reason
            that communication over a virtual Linux switch is limited to a
            single Linux host, while our containers are distributed across many
            nodes.
          </p>

          <h4 id="Using-Overlay-Networks">7.1.1 Using Overlay Networks</h4>

          <p>
            Instead, Mothership makes use of an <em>overlay network</em> for
            connecting services distributed across the fleet. Ultimately, the
            nodes in our fleet are not connected to each other over a physical
            LAN – they must communicate over the Internet. But, by using Linux
            VXLAN technology, it is possible to create a <em>virtual</em> layer
            2 subnet that spans a distributed layer 3 physical network (i.e.,
            the Internet).
            <a
              href="https://www.juniper.net/us/en/products-services/what-is/vxlan/"
              target="_blank"
              rel="noopener"
              >link</a
            >
            This is accomplished by tunneling layer 2 Ethernet frames inside of
            layer 3 UDP packets. This virtual LAN <em>overlays</em> the
            underlying physical network, giving us the isolation of a layer 2
            subnet even though our containers (or their host nodes) are not
            actually physically connected at the link layer.<sup
              class="footnote-ref"
              ><a href="#fn6" id="fnref6">[6]</a></sup
            >
          </p>

          <p>
            Putting this theory into practice, Mothership creates a separate
            overlay network for each app service. When a user adds a database to
            their app, Mothership connects this database to the app’s overlay
            network. This enables communication between the app and its
            database, but keeps the database service isolated from both other
            applications and the outside world. For example, in the following
            diagram, we have two apps, app1 and app2, each connected to its own
            overlay network spanning multiple app pods. The databases for each
            app are also connected to these networks, allowing them to
            communicate with their respective app:
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/app-overlay.png"
              style="max-height: 500px;"
              alt="Detailed comparison showing abstractions"
            />
          </div>

          <p>
            While secure communication between distributed services is now
            theoretically possible, we still haven’t solved the service
            discovery problem: how does the app service know the IP address of
            the database service on their shared overlay network? The answer is
            DNS. Service names are mapped to container IP addresses via a DNS
            server local to the overlay network. Whenever our container
            orchestrator creates a new container for a service, it updates this
            DNS server.<sup class="footnote-ref"
              ><a href="#fn7" id="fnref7">[7]</a></sup
            >
          </p>
          <p>
            What this means is that app services only need to know the
            <em>service name</em> of their associated databases to send requests
            to them across the overlay network.
          </p>

          <h3 id="Handling-External-Requests">
            7.2 Handling External Requests
          </h3>

          <p>
            Overlay networks solve the problem of inter-service communication.
            But now we need to solve the problem of mapping external requests to
            running containers. Ideally, we’d like deployed applications to be
            accessible via subdomain URLs (e.g., <code>my-app</code> could be
            viewed by visiting <code>http://my-app.my-paas.com</code>). But this
            is not so simple in our distributed, multi-tenant architecture.
            Consider the following diagram:
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/mapping-urls.png"
              style="max-height: 500px;"
              alt="How do you map services to URLs?"
            />
          </div>

          <p>
            Here, a request comes into our system to for
            <code>app7.my-paas.com</code>. As the diagram reveals, to actually
            service this request we need answers to the following questions:
          </p>

          <ol>
            <li>Which app pod currently has a container running for app7?</li>
            <li>
              How will that app pod forward the request to the appropriate
              container?
            </li>
          </ol>

          <p>
            Neither question has a trivial answer. Our container orchestrator
            moves app containers among pods as needed, so the answer to the
            first question is a constantly moving target. And our app containers
            themselves are not yet exposed to the Internet – they are only
            accessible from within their private overlay networks.
          </p>

          <p>
            Docker comes with a built-in tool for answering these questions
            called the <em>ingress routing mesh</em>. Unfortunately, as we
            explain in the next section, this tool is not appropriate for our
            use-case.
          </p>

          <h4 id="Ingress-Routing-Mesh">7.2.1 Ingress Routing Mesh?</h4>

          <p>
            In Docker, the standard approach for exposing containers to the
            Internet is <em>port publishing</em>. When starting a container, one
            can request that Docker forward traffic from one of the host’s
            external ports to a particular port inside the container.<sup
              class="footnote-ref"
              ><a href="#fn8" id="fnref8">[8]</a></sup
            >
            For example, we could configure that requests to the host’s port
            8080 be mapped to a particular container’s port 80:
          </p>

          <div class="img-wrapper">
            <img
              style="max-height: 10rem;"
              src="images/diagrams/port-publishing.png"
              alt="Detailed comparison showing abstractions"
            />
          </div>

          <p>
            Port publishing allows our an app pod to forward requests for an
            application to the correct container. So once an app pod receives a
            request, it can pass it along to the correct container. But how do
            we know which app pod to send a particular request to?
          </p>

          <p>
            This is where Docker’s ingress routing mesh comes into play. The
            ingress routing mesh includes a layer 4 load balancer. This load
            balancer is managed by the container orchestrator, and knows which
            pods have ports listening for each app. Therefore, when it receives
            a request on a particular port, it can forward the request to the
            same port on the correct pod:
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/ingress-routing.png"
              alt="Detailed comparison showing abstractions"
            />
          </div>

          <p>
            Unfortunately, this solution does not fit our use-case. The problem
            is the sort of URLs this will provide. We’d need to configure our
            DNS so that <code>my-paas</code> resolves to one of the IP addresses
            in our cluster; let’s say we point it at the orchestrator for
            simplicity. To visit a particular app, we’d need to know the port it
            was published on, and then the URL would be something like
            <code>http://my-paas.com:54731</code>. This is not a memorable URL,
            and would appear strange to users who are not familiar with ports.
            Additionally, the solution requires opening lots of ports on our
            servers, which is a security vulnerability.
          </p>

          <h4 id="Adding-a-Reverse-Proxy-Service">
            7.2.2 Adding a Reverse Proxy Service
          </h4>

          <p>
            We need some sort of load balancer/proxy, but what Docker provides
            is a layer 4 load balancer, which requires port numbers in URLs. We,
            however, want human-readable URLs with app names as subdomains (e.g.
            <code>app7.my-paas.com</code>). This means that we need our proxy to
            forward requests based on <em>hostname</em>, which are an
            application layer concern. So our architecture requires a
            <em>layer 7</em> reverse proxy service, such as HAProxy or Nginx.
            Adding that, our architecture is as follows:
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/proxy-no-overlay.png"
              alt="Detailed comparison showing abstractions"
            />
          </div>

          <p>
            Our reverse proxy must somehow map URLs to containers. As we learned
            in the section on inter-service communication, this cannot be done
            by container IP address since these are ephemeral. Indeed, this
            problem is just another instance of the problem of inter-service
            communication we solved earlier. And so Mothership solves it in much
            the same way.
          </p>

          <p>
            First, we add another overlay network for our proxy service. Then,
            each time an app is deployed, it is connected not only to its own
            overlay network, but also to the proxy overlay network, as we see in
            the following diagram:
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/proxy-network-with-two-containers.png"
              style="max-height: 500px;"
              alt="Diagram showing apps connected to proxy network"
            />
          </div>

          <p>
            The Mothership proxy service can now communicate directly with apps
            via their service name. This means that each time an app is
            deployed, we can add a configuration line to our proxy to map a URL
            to the app service, along the following lines:
          </p>

          <ul>
            <li>
              <code>http://my-app.my-paas.com</code> →
              <code>my-app_service</code>
            </li>
          </ul>

          <p>
            Essentially, the proxy service forwards a request for
            <code>my-app.my-paas.com</code> to the local hostname
            <code>my-app_service</code>. The proxy overlay network’s local DNS
            server ensures that <code>my-app_service</code> resolves to the
            current IP of a container for <code>my-app</code>.
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/apps-to-proxy-solved.png"
              style="max-height: 500px;"
              alt="Incoming request hit proxy, go through proxy overlay, and then to app"
            />
          </div>

          <p>
            An added benefit of this architecure is that app containers
            themselves are not directly exposed to the Internet, isolating
            security concerns to the proxy service. Although our apps don’t need
            to be directly exposed to the Internet, the Mothership
            <em>proxy</em> does. In this case, we’ll need to expose a port.
            Since the proxy is fielding HTTP/S requests, Mothership publishes it
            on ports 80/443. Since the proxy internally decides which subdomains
            go to which services, we simply map a wildcard subdomain
            (<code>*.my-paas.com</code>) to our orchestrator node. Incoming web
            requests will hit that node on port 80 or 443, and the ingress
            routing mesh ensures that it gets forwarded to our proxy service,
            and the proxy then sends it to the appropriate container.
          </p>

          <p>
            And with that, Mothership’s service discovery challenges are solved.
          </p>

          <ol>
            <li>
              Apps can communicate with their databases using overlay networks,
              without exposing the databases to the outside world
            </li>
            <li>
              External requests to apps are routed to running containers via an
              L7 reverse proxy and an additional overlay network
            </li>
          </ol>

          <hr />

          <p>
            At this point, it’s worth briefly reviewing Mothership’s core
            architecture:
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/overall-architecture.png"
              style="max-height: 500px;"
              alt="Incoming request hit proxy, go through proxy overlay, and then to app"
            />
          </div>

          <p>
            Arriving at this architecture, and solving the many problems it
            entails, was a major challenge in the development of Mothership. But
            for our PaaS to be useful to teams looking to deploy internal-facing
            apps, many features needed to be added, and each involved
            significant engineering challenges of its own. We turn to these
            features and challenges next.
          </p>

          <h2 id="Essential-Features" class="sec-header">
            8. Essential Features
          </h2>

          <h3 id="Running-Terminal-Commands">8.1 Running Terminal Commands</h3>

          <p>
            A common deployment need is the ability to run post-deployment
            commands. In particular, most deployments will need the ability to
            run database migrations once the application code is deployed and
            the database is provisioned. There are a variety of ways this can be
            accomplished, but we decided that the simplest and most powerful is
            to give developers direct terminal access to their deployed
            applications.
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/gifs/rake-db-migrate.gif"
              alt="Animation showing database migration inside a console"
            />
          </div>

          <p>
            A benefit of this approach is it gives developers an intuitive way
            to inspect exactly what the deployed source code looks like. This
            can be very valuable in debugging deploys to ensure that the correct
            files made it onto Mothership.
          </p>

          <p>
            Giving users the ability to interact with containers in the fleet
            through an in-browser terminal emulator turned out to be a very
            interesting challenge. The first problem is deciding which container
            to connect to. A naive approach would be to connect the terminal
            emulator to one of the already running application containers that
            is servicing web requests. There are several downsides with this
            approach:
          </p>

          <ul>
            <li>
              Which containers are running for a service and where is an
              internal concern of our container orchestrator, and not something
              we have easy access to
            </li>
            <li>
              A user might accidentally run a harmful command and break their
              deployed application (e.g., <code>rm -rf ./</code>)
            </li>
            <li>
              The job of an application container started by our orchestrator is
              to serve web requests for the application – we shouldn’t burden it
              with extra load
            </li>
          </ul>

          <p>
            Mothership takes a different approach. Instead of finding and
            connecting to an existing container for the application in question,
            we create a new “one-off container.” This one-off container is based
            on the same Docker image that we created when the app was deployed,
            so it is functionally identical to the running containers serving
            requests. We call it a “one-off” container because its sole purpose
            is to handle a single terminal session – once the session ends, the
            container is destroyed to free up resources.
          </p>

          <p>
            Instead of starting one-off containers with whatever command would
            boot the application, we start them with the
            <code>bash</code> command. This starts a shell process and allocates
            a TTY for the container. Recall that Mothership’s mission control
            service connects to the remote Docker host in the fleet to create
            services and containers. If mission control is to send terminal
            output back to a client, it will somehow need access to a remote
            container’s stdin and stdout streams. Fortunately, the Docker API
            makes this possible: we can upgrade our initial HTTP request into a
            persistent, full-duplex TCP connection over which we can stream to
            and from the container’s stdin and stdout.
          </p>

          <p>
            Mission control interacts with this stream as a Node.js
            <code>Duplex</code> stream object. The next challenge is to expose
            this stream to clients, so that they can send terminal commands over
            it and receive terminal output from it.
          </p>

          <p>
            Since we’re looking for a persistent, bi-directional connection
            between a browser and our server, the WebSocket protocol is ideal
            for our use-case. When a user requests to start a terminal session
            with their app from a browser, the Mothership web client sends a
            WebSocket request to mission control. Mission control issues a
            request to the remote Docker host on our orchestrator to create a
            suitable one-off container, and stream its stdin/stdout over TCP.
            Mission control then pipes any data coming in from the WebSocket to
            the container’s stdin stream, and any data coming from the
            container’s stdout stream out over the WebSocket. In essence,
            mission control serves as a proxy between clients and the containers
            they’re interacting with.
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/ws-terminal-print.png"
              alt="Showing how ephemeral containers are made for console use"
            />
          </div>

          <p>
            To accept client input and output, a simple text field will not
            suffice. Along with regular character data, the shell will send
            <a
              href="https://en.wikipedia.org/wiki/ANSI_escape_code"
              target="_blank"
              rel="noopener"
              >ANSI escape sequences</a
            >
            for formatting output that the client must interpret. The Mothership
            client uses the Xterm.js terminal emulator to enable this. But what
            about handling and displaying input?
          </p>

          <p>
            We initially tried directly display characters the user enters, and
            then sending them over the WebSocket once the user hits enters. It
            quickly became apparent that this was not the correct approach. For
            one thing, the same character can have different meanings in
            different contexts. At a shell prompt, entering
            <code>q</code> should cause the letter “q” to be displayed in the
            prompt. But inside a <code>less</code> process, for example,
            <code>q</code> should quit the process. Additionally, entering an
            escape sequence like <code>ctrl-C</code> should have an effect
            without pressing the enter key. In general, it is the shell’s job to
            interpret keystrokes and determine the result – whether that’s
            sending a character to stdout or terminating a process.
          </p>

          <p>
            So Mothership’s client-side JavaScript directly sends each keystroke
            over the WebSocket and into the container, and only displays output
            when it receives stdout data from the WebSocket. This may sound like
            a lot of overhead for each keystroke, but it is actually quite
            performant due to the nature of WebSockets.
          </p>

          <p>
            The final piece to this puzzle is security. The Mothership client
            requires authentication, so that only authorized users can interact
            with it. We accomplish this via Express middleware. Unfortunately
            for us, Node’s HTTP server handles WebSocket upgrade requests
            directly and never forwards them to our Express app. This means that
            our Express middleware is never given the chance to authenticate
            upgrade requests. Without some protective measures, anyone with
            access to our WebSocket URLs would be able to establish a terminal
            session with deployed applications – a major security vulnerability.
          </p>

          <p>
            To prevent this, when an upgrade request comes in, our WebSocket
            handlers sends another a GET request to our Express application with
            all authentication headers (cookies or bearer tokens). If the
            Express application returns a 200 response, authentication was
            successful and the upgrade proceeds. If it returns a 401 response,
            authentication failed and we reject the upgrade request.
          </p>

          <h3 id="Health-Checks">8.2 Health Checks</h3>

          <p>
            Users of a PaaS expect some way of viewing the health of deployed
            applications – the PaaS should be able to tell whether an
            application is running in a healthy state. There are many ways of
            monitoring the health of an application, but Mothership uses a
            simple, but useful metric. If our container orchestrator has
            restarted an application more than 5 times in the last minute, this
            indicates a problem. A variety of things can cause this – more load
            than a single app server can handle, application errors that crash
            the server, or configuration errors like forgetting to install the
            <code>puma</code> gem for an app attempting to boot using the
            <code>puma</code> command.
          </p>

          <p>
            When this is happening, Mothership lists the app as unhealthy,
            otherwise it is considered healthy. The number 5 is somewhat
            arbitrary, but in our testing, a small handful of restarts is
            normal, but more than 5 is highly correlated with a problem that is
            preventing the app from handling requests.
          </p>

          <p>
            Once again, we use WebSockets to make health statuses visible to
            user. When a user visits an application’s “show” page, the web
            client establishes a WebSocket connection with mission control.
            Mission control then sets a timeout that periodically checks with
            the orchestrator for application restarts, and sends the result
            (<code>healthy</code> or <code>unhealthy</code>) to the client over
            the WebSocket. This result is rendered client-side with a dynamic
            badge.
          </p>

          <div class="img-wrapper">
            <img
              src="images/diagrams/health-checks.png"
              style="max-height: 100px;"
              alt="Showing dynamically generated health check buttons that say healthy or unhealthy"
            />
          </div>

          <h3 id="Service-Logs">8.3 Service Logs</h3>

          <p>
            Another critical feature is the ability to view applications logs.
            For example, if an application has an <code>unhealthy</code> status,
            a user will want to look at its logs to identify the problem.
            Because we an application can potentially can have many instances,
            there is no particular <em>container</em> we want to check for logs.
            Instead, we want the logs for the entire application service.
            Fortunately, our container orchestrator happens to aggregate logs
            for an app service from all running containers, and we can request
            these via the Docker API.
          </p>

          <p>
            To present these to clients, we employ a strategy much like we did
            for terminal sessions. Mission control establishes a direct TCP
            connection with the container orchestrator, which it receives in a
            Node.js stream. Any new logs for the relevant app service arrive in
            this stream. Client-side, we display another terminal emulator, and
            establish a WebSocket connection to special endpoint in our
            WebSocket handler. Mission Control pipes output from its service
            logs stream over the WebSocket connection, and it appears in the
            client’s terminal emulator.
          </p>

          <h3 id="Database-Backups">8.4 Database Backups</h3>

          <p>
            Mothership allows users to perform manual backups of application
            databases. Our basic strategy uses PostgreSQL’s
            <code>pg_dump</code> utility. This utility dumps the SQL statements
            needed to restore the database to its complete state as of the time
            the command is run. It can be run while there are concurrent
            connections to the database, and guarantees a consistent snapshot
            (even as concurrent updates are occurring, details). The basic
            strategy is to run this command in a container with access to the
            live database – but the actual implementation is surprisingly
            complicated.
          </p>

          <p>
            But where do we run <code>pg_dump</code>, and how do we get the
            output back to the client? Once again, we make use of a one-off
            container. But a container based on an app image won’t work because
            these images don’t have PostgreSQL installed in them. Instead, we
            use the official Postgres image.
          </p>

          <p>
            The next step is somewhat tricky. We cannot simply start the
            container with the <code>pg_dump</code> command. If we do that, we
            override the image’s default command which starts the Postgres
            daemon – and we can’t run <code>pg_dump</code> if there’s no
            Postgres daemon. To solve this problem, we attach the one-off
            Postgres container to the app’s network so it can talk to the live
            database server. <code>pg_dump</code> can connect to remote postgres
            servers and run against them: so we run <code>pg_dump</code> to
            start the container, but tell it to connect to the app’s running
            database container.
          </p>

          <p>
            How do we get this dump back to the client? By default,<code
              >pg_dump</code
            >
            sends its output to <code>stdout</code>, although you would normally
            redirect this to a file. Since our PaaS server is connected to this
            container via a TCP connection with the Docker host, its stdout is
            available to us as a stream on the PaaS. So we let pg_dup stream its
            output to stdout in its container, and then we capture that stream
            in the PaaS and redirect it into a temporary <code>.sql</code> file.
            Once the stream ends, we remove the temporary postgres container,
            send the file to the client, and then remove the temporary file.
          </p>

          <h3 id="Deploying-and-Scaling-Mothership">
            8.5 Deploying and Scaling Mothership
          </h3>

          <p>
            One of the downsides of an open-source PaaS is that users must
            manage it themselves. In particular, the tasks of initially deploy a
            PaaS has the potential to be more work than it’s worth, as does the
            task of scaling the PaaS as the number of deployed applicaitons
            grows. One of the driving goals of Mothership was to make these
            tasks as easy as possible.
          </p>

          <p>
            Let’s first examine Mothership’s strategy for handling scaling. From
            the CLI, a user can run the command
            <code>mothership cluster-scale</code>. This adds an additional
            app-pod to the convoy. The container orchestrator will take care of
            this pod as it sees fit. Easy. But there are a number of not-so-easy
            steps that must be completed to enable this:
          </p>

          <ol>
            <li>We must provision a new server with the user’s IaaS account</li>
            <li>We need to install Docker on this server</li>
            <li>
              We need to register this new server with our container
              orhcestrator
            </li>
          </ol>

          <p>
            Mothership uses Docker Machine to handle server provisioning. Docker
            Machine comes with “providers” for many IaaS providers (Digital
            Ocean, Linode, AWS, and many others). These providers do the work of
            sending the requisite API calls to the IaaS provider in question to
            get a server provisioned with Docker installed and the correct ports
            open. By using Docker Machine, we freed ourselves to work at a
            higher level of abstraction in implementing scaling. Once we have
            this new Docker host, we can send it an API request instructing it
            to register itself on our Convoy.<sup class="footnote-ref"
              ><a href="#fn9" id="fnref9">[9]</a></sup
            >
          </p>
          <h2 id="Future-Work" class="sec-header">7. Future Work</h2>
          <p>
            Venice meets many of the needs of developers new to event-stream
            processing. Venice provides a quick-to-deploy framework that has
            much of the configuration work automated. This means developers can
            get to their tasks of connecting their own data producers and
            implementing queries over their data in minutes. They can also
            verify things are running smoothly using the graphical user
            interface and CLI.
          </p>
          <p>
            However, there are always areas for improvement. The following goals
            would likely add the most value to Venice:
          </p>
          <ul>
            <li>
              Automate deployment and management across distributed servers
            </li>
            <li>Enable auto-scaling (up and down) according to workload</li>
            <li>Add more default pipelines</li>
            <li>Add support for more connectors</li>
            <li>Enable external network storage</li>
            <li>Automate the process of adding producers to the pipeline</li>
          </ul>

          <h2 id="References" class="sec-header">8. References</h2>

          <ol>
            <li id="microservices">
              S. Newman, Building Microservices, 1st ed. Sebastopol, CA, USA.
              O’Reilly, 2015.
            </li>
            <li id="mssp">
              M. Kleppman, Making Sense of Stream Processing, 1st ed.
              Sebastopol, CA, USA. O’Reilly, 2016.
            </li>
            <li id="ddia">
              M. Kleppmann, Designing Data Intensive Applications, 1st ed.
              Sebastopol, CA, USA. O’Reilly, 2017.
            </li>
            <li id="two-phase-commit">
              “Consensus Protocols: Two-Phase-Commit”, The Paper Trail blog,
              <a
                href="https://www.the-paper-trail.org/post/2008-11-27-consensus-protocols-two-phase-commit/"
                target="_blank"
                >https://www.the-paper-trail.org/post/2008-11-27-consensus-protocols-two-phase-commit/</a
              >
              (April 13, 2020).
            </li>
            <li id="okay-store-data">
              J. Kreps, “It’s Okay To Store Data In Apache Kafka”, Confluent.io
              blog,
              <a
                href="https://www.confluent.io/blog/okay-store-data-apache-kafka/"
                target="_blank"
                >https://www.confluent.io/blog/okay-store-data-apache-kafka/</a
              >
              (April 16, 2020).
            </li>
            <li id="streaming-systems">
              T. Akidau, S. Chernyak, and R. Lax, Streaming Systems, 1st ed.
              Sebastopol, CA, USA. O’Reilly, 2018.
            </li>
            <li id="kafka-streams">
              J. Kreps, “Introducing Kafka Streams: Stream Processing Made
              Simple”, Confluent.io blog,
              <a
                href="https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/"
                target="_blank"
                >https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/</a
              >
              (April 14, 2020).
            </li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
          </ol>

          <hr class="footnotes-sep" />

          <div class="footnotes">
            <ol class="footnotes-list">
              <li id="fn1" class="footnote-item">
                <p>
                  A container orchestrator has several other critical
                  responsibilities, as we’ll see in the next section on service
                  discovery. <a href="#fnref1" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn2" class="footnote-item">
                <p>
                  Officially, Docker Swarm was a separate product from Docker
                  that is now deprecated, while <em>Swarm Mode</em> is the
                  current container orchestrator built into Docker as of v1.12.
                  Whenever we say “Docker Swarm”, we are referring to “Docker in
                  Swarm Mode.” <a href="#fnref2" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn3" class="footnote-item">
                <p>
                  Manager nodes in Docker Swarm are convenient in that they can
                  serve dual-duty as worker nodes. This makes it possible to
                  have a cluster with a single node, which is how Mothership
                  operates on first setup.
                  <a href="#fnref3" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn4" class="footnote-item">
                <p>
                  There is a subtle distinction between a task and a container
                  that is immaterial to the present discussion.
                  <a href="#fnref4" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn5" class="footnote-item">
                <p>
                  Since containers are ephemeral, but database data is
                  persistent, we make use of Docker volumes to persist database
                  data. <a href="#fnref5" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn6" class="footnote-item">
                <p>
                  The implementation details of Docker overlay networks get
                  pretty hairy, but
                  <a
                    href="https://success.docker.com/article/networking#overlaydrivernetworkarchitecture"
                    target="_blank"
                    rel="noopener"
                    >this reference article</a
                  >
                  is a good place to start.
                  <a href="#fnref6" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn7" class="footnote-item">
                <p>
                  This is an oversimplification because there may be
                  <em>multiple</em> running containers for a given service.
                  Docker Swarm uses DNS round-robin load-balancing to distribute
                  requests to services among running containers.
                  <a href="#fnref7" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn8" class="footnote-item">
                <p>
                  Under the hood, Docker accomplishes this via heavy use of
                  Linux iptables rules.
                  <a href="#fnref8" class="footnote-backref">↩︎</a>
                </p>
              </li>
              <li id="fn9" class="footnote-item">
                <p>
                  We don’t go into details here due to space, but even here
                  there were surprising complexities. For example, scaling a
                  Convoy <em>down</em> requires telling a pod to leave the
                  swarm, and then telling the Convoy to delete that pod. This
                  requires two Docker API calls. Unfortunately, the call for the
                  pod to leave returns a response before it has fully left, but
                  the call for the Convoy to delete it can only succeed after it
                  has fully left – an unfortunate race condition with no clean
                  solution. <a href="#fnref9" class="footnote-backref">↩︎</a>
                </p>
              </li>
            </ol>
          </div>
        </section>
      </main>
    </div>
    <section class="lower-cta">
      <h2>Ready to get started?</h2>

      <div class="buttons">
        <a href="docs.html" class="button button-link button-wide banner-cta"
          >Docs</a
        >
        <a href="" class="button button-primary button-wide banner-cta"
          >Quick Start</a
        >
        <!-- TODO: add link to quick start -->
        <a
          href="https://github.com/venice-framework"
          target="_blank"
          class="button button-link banner-cta"
          >GitHub</a
        >
      </div>
    </section>

    <footer>
      <div class="footer-backdrop">
        <section id="our-team">
          <h2>Our Team</h2>
          <p>
            We are looking for opportunities.<br />If you liked what you saw and
            want to talk, please reach out!
          </p>
          <ul>
            <li class="individual">
              <img src="images/team/nancy.png" alt="Nancy Trinh" />
              <h3>Nancy Trinh</h3>
              <p>San Francisco, CA</p>
              <ul class="social-icons">
                <li>
                  <a
                    href="mailto:nancytrinh20@gmail.com?subject=Venice%20Project"
                  >
                    <img src="images/icons/email_icon.png" alt="email" />
                  </a>
                </li>
                <li>
                  <a href="http://github.com/nantrinh" target="_blank">
                    <img src="images/icons/website_icon.png" alt="website" />
                  </a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/nancytrinh/"
                    target="_blank"
                  >
                    <img src="images/icons/linked_in_icon.png" alt="linkedin" />
                  </a>
                </li>
              </ul>
            </li>
            <li class="individual">
              <img src="images/team/David.jpg" alt="David Perich" />
              <h3>David Perich</h3>
              <p>Melbourne, AU</p>
              <ul class="social-icons">
                <li>
                  <a
                    href="mailto:davidnperich@gmail.com?subject=Venice%20Project"
                  >
                    <img src="images/icons/email_icon.png" alt="email" />
                  </a>
                </li>
                <li>
                  <a href="http://www.davidperich.com" target="_blank">
                    <img src="images/icons/website_icon.png" alt="website" />
                  </a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/davidperich/"
                    target="_blank"
                  >
                    <img src="images/icons/linked_in_icon.png" alt="linkedin" />
                  </a>
                </li>
              </ul>
            </li>
            <li class="individual">
              <img src="images/team/Melissa.png" alt="Melissa Manousos" />
              <h3>Melissa Manousos</h3>
              <p>Los Angeles, CA</p>
              <ul class="social-icons">
                <li>
                  <a
                    href="mailto:me@melissamanousos.com?subject=Venice%20Project"
                    target="_blank"
                  >
                    <img src="images/icons/email_icon.png" alt="email" />
                  </a>
                </li>
                <li>
                  <a href="https://melissamanousos.com/" target="_blank">
                    <img src="images/icons/website_icon.png" alt="website" />
                  </a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/melissa-manousos"
                    target="_blank"
                  >
                    <img src="images/icons/linked_in_icon.png" alt="linkedin" />
                  </a>
                </li>
              </ul>
            </li>
          </ul>
        </section>
      </div>
      <div class="design-credit">
        Logo and color scheme:
        <a href="http://linzimurray.creative" target="_blank"
          >linzimurray.creative</a
        >
        • Layout: <a href="http://jkulton.com" target="_blank">Jon Kulton</a>
      </div>
    </footer>
  </body>
</html>
